Index: linux-latency/drivers/md/bitmap.c
===================================================================
--- linux-latency.orig/drivers/md/bitmap.c
+++ linux-latency/drivers/md/bitmap.c
@@ -27,11 +27,13 @@
 #include <linux/mount.h>
 #include <linux/buffer_head.h>
 #include <linux/seq_file.h>
+#include <linux/device-mapper.h>
 #include "md.h"
 #include "bitmap.h"
 
 static inline char *bmname(struct bitmap *bitmap)
 {
+	dm_enter
 	return bitmap->mddev ? mdname(bitmap->mddev) : "mdX";
 }
 
@@ -52,6 +54,7 @@ __acquires(bitmap->lock)
 {
 	unsigned char *mappage;
 
+	dm_enter
 	if (page >= bitmap->pages) {
 		/* This can happen if bitmap_start_sync goes beyond
 		 * End-of-device while looking for a whole page.
@@ -103,6 +106,7 @@ static void bitmap_checkfree(struct bitm
 {
 	char *ptr;
 
+	dm_enter
 	if (bitmap->bp[page].count) /* page is still busy */
 		return;
 
@@ -138,6 +142,7 @@ static int read_sb_page(struct mddev *md
 	struct md_rdev *rdev;
 	sector_t target;
 
+	dm_enter
 	rdev_for_each(rdev, mddev) {
 		if (! test_bit(In_sync, &rdev->flags)
 		    || test_bit(Faulty, &rdev->flags))
@@ -157,6 +162,7 @@ static int read_sb_page(struct mddev *md
 
 static struct md_rdev *next_active_rdev(struct md_rdev *rdev, struct mddev *mddev)
 {
+	dm_enter
 	/* Iterate the disks of an mddev, using rcu to protect access to the
 	 * linked list, and raising the refcount of devices we return to ensure
 	 * they don't disappear while in use.
@@ -193,6 +199,7 @@ static int write_sb_page(struct bitmap *
 	struct mddev *mddev = bitmap->mddev;
 	struct bitmap_storage *store = &bitmap->storage;
 
+	dm_enter
 	while ((rdev = next_active_rdev(rdev, mddev)) != NULL) {
 		int size = PAGE_SIZE;
 		loff_t offset = mddev->bitmap_info.offset;
@@ -264,6 +271,7 @@ static void write_page(struct bitmap *bi
 {
 	struct buffer_head *bh;
 
+	dm_enter
 	if (bitmap->storage.file == NULL) {
 		switch (write_sb_page(bitmap, page, wait)) {
 		case -EINVAL:
@@ -293,6 +301,7 @@ static void end_bitmap_write(struct buff
 {
 	struct bitmap *bitmap = bh->b_private;
 
+	dm_enter
 	if (!uptodate)
 		set_bit(BITMAP_WRITE_ERROR, &bitmap->flags);
 	if (atomic_dec_and_test(&bitmap->pending_writes))
@@ -303,6 +312,7 @@ static void end_bitmap_write(struct buff
 static void
 __clear_page_buffers(struct page *page)
 {
+	dm_enter
 	ClearPagePrivate(page);
 	set_page_private(page, 0);
 	page_cache_release(page);
@@ -311,6 +321,7 @@ static void free_buffers(struct page *pa
 {
 	struct buffer_head *bh;
 
+	dm_enter
 	if (!PagePrivate(page))
 		return;
 
@@ -341,6 +352,7 @@ static int read_page(struct file *file,
 	struct buffer_head *bh;
 	sector_t block;
 
+	dm_enter
 	pr_debug("read bitmap file (%dB @ %llu)\n", (int)PAGE_SIZE,
 		 (unsigned long long)index << PAGE_SHIFT);
 
@@ -401,6 +413,7 @@ void bitmap_update_sb(struct bitmap *bit
 {
 	bitmap_super_t *sb;
 
+	dm_enter
 	if (!bitmap || !bitmap->mddev) /* no bitmap for this array */
 		return;
 	if (bitmap->mddev->bitmap_info.external)
@@ -431,6 +444,7 @@ void bitmap_print_sb(struct bitmap *bitm
 {
 	bitmap_super_t *sb;
 
+	dm_enter
 	if (!bitmap || !bitmap->storage.sb_page)
 		return;
 	sb = kmap_atomic(bitmap->storage.sb_page);
@@ -471,6 +485,7 @@ static int bitmap_new_disk_sb(struct bit
 	bitmap_super_t *sb;
 	unsigned long chunksize, daemon_sleep, write_behind;
 
+	dm_enter
 	bitmap->storage.sb_page = alloc_page(GFP_KERNEL);
 	if (bitmap->storage.sb_page == NULL)
 		return -ENOMEM;
@@ -535,6 +550,7 @@ static int bitmap_read_sb(struct bitmap
 	int err = -EINVAL;
 	struct page *sb_page;
 
+	dm_enter
 	if (!bitmap->storage.file && !bitmap->mddev->bitmap_info.offset) {
 		chunksize = 128 * 1024 * 1024;
 		daemon_sleep = 5 * HZ;
@@ -652,6 +668,7 @@ out_no_sb:
 static inline unsigned long file_page_index(struct bitmap_storage *store,
 					    unsigned long chunk)
 {
+	dm_enter
 	if (store->sb_page)
 		chunk += sizeof(bitmap_super_t) << 3;
 	return chunk >> PAGE_BIT_SHIFT;
@@ -661,6 +678,7 @@ static inline unsigned long file_page_in
 static inline unsigned long file_page_offset(struct bitmap_storage *store,
 					     unsigned long chunk)
 {
+	dm_enter
 	if (store->sb_page)
 		chunk += sizeof(bitmap_super_t) << 3;
 	return chunk & (PAGE_BITS - 1);
@@ -676,6 +694,7 @@ static inline unsigned long file_page_of
 static inline struct page *filemap_get_page(struct bitmap_storage *store,
 					    unsigned long chunk)
 {
+	dm_enter
 	if (file_page_index(store, chunk) >= store->file_pages)
 		return NULL;
 	return store->filemap[file_page_index(store, chunk)
@@ -689,6 +708,7 @@ static int bitmap_storage_alloc(struct b
 	unsigned long num_pages;
 	unsigned long bytes;
 
+	dm_enter
 	bytes = DIV_ROUND_UP(chunks, 8);
 	if (with_super)
 		bytes += sizeof(bitmap_super_t);
@@ -740,6 +760,7 @@ static void bitmap_file_unmap(struct bit
 	int pages;
 	struct file *file;
 
+	dm_enter
 	file = store->file;
 	map = store->filemap;
 	pages = store->file_pages;
@@ -770,6 +791,7 @@ static void bitmap_file_kick(struct bitm
 {
 	char *path, *ptr = NULL;
 
+	dm_enter
 	if (!test_and_set_bit(BITMAP_STALE, &bitmap->flags)) {
 		bitmap_update_sb(bitmap);
 
@@ -801,24 +823,28 @@ enum bitmap_page_attr {
 static inline void set_page_attr(struct bitmap *bitmap, int pnum,
 				 enum bitmap_page_attr attr)
 {
+	dm_enter
 	set_bit((pnum<<2) + attr, bitmap->storage.filemap_attr);
 }
 
 static inline void clear_page_attr(struct bitmap *bitmap, int pnum,
 				   enum bitmap_page_attr attr)
 {
+	dm_enter
 	clear_bit((pnum<<2) + attr, bitmap->storage.filemap_attr);
 }
 
 static inline int test_page_attr(struct bitmap *bitmap, int pnum,
 				 enum bitmap_page_attr attr)
 {
+	dm_enter
 	return test_bit((pnum<<2) + attr, bitmap->storage.filemap_attr);
 }
 
 static inline int test_and_clear_page_attr(struct bitmap *bitmap, int pnum,
 					   enum bitmap_page_attr attr)
 {
+	dm_enter
 	return test_and_clear_bit((pnum<<2) + attr,
 				  bitmap->storage.filemap_attr);
 }
@@ -836,6 +862,7 @@ static void bitmap_file_set_bit(struct b
 	void *kaddr;
 	unsigned long chunk = block >> bitmap->counts.chunkshift;
 
+	dm_enter
 	page = filemap_get_page(&bitmap->storage, chunk);
 	if (!page)
 		return;
@@ -860,6 +887,7 @@ static void bitmap_file_clear_bit(struct
 	void *paddr;
 	unsigned long chunk = block >> bitmap->counts.chunkshift;
 
+	dm_enter
 	page = filemap_get_page(&bitmap->storage, chunk);
 	if (!page)
 		return;
@@ -885,6 +913,7 @@ void bitmap_unplug(struct bitmap *bitmap
 	int dirty, need_write;
 	int wait = 0;
 
+	dm_enter
 	if (!bitmap || !bitmap->storage.filemap ||
 	    test_bit(BITMAP_STALE, &bitmap->flags))
 		return;
@@ -940,6 +969,7 @@ static int bitmap_init_from_disk(struct
 	void *paddr;
 	struct bitmap_storage *store = &bitmap->storage;
 
+	dm_enter
 	chunks = bitmap->counts.chunks;
 	file = store->file;
 
@@ -1058,6 +1088,7 @@ void bitmap_write_all(struct bitmap *bit
 	 */
 	int i;
 
+	dm_enter
 	if (!bitmap || !bitmap->storage.filemap)
 		return;
 	if (bitmap->storage.file)
@@ -1075,6 +1106,7 @@ static void bitmap_count_page(struct bit
 {
 	sector_t chunk = offset >> bitmap->chunkshift;
 	unsigned long page = chunk >> PAGE_COUNTER_SHIFT;
+	dm_enter
 	bitmap->bp[page].count += inc;
 	bitmap_checkfree(bitmap, page);
 }
@@ -1085,6 +1117,7 @@ static void bitmap_set_pending(struct bi
 	unsigned long page = chunk >> PAGE_COUNTER_SHIFT;
 	struct bitmap_page *bp = &bitmap->bp[page];
 
+	dm_enter
 	if (!bp->pending)
 		bp->pending = 1;
 }
@@ -1106,6 +1139,7 @@ void bitmap_daemon_work(struct mddev *md
 	sector_t blocks;
 	struct bitmap_counts *counts;
 
+	dm_enter
 	/* Use a mutex to guard daemon_work against
 	 * bitmap_destroy.
 	 */
@@ -1236,6 +1270,7 @@ __acquires(bitmap->lock)
 	sector_t csize;
 	int err;
 
+	dm_enter
 	err = bitmap_checkpage(bitmap, page, create);
 
 	if (bitmap->bp[page].hijacked ||
@@ -1264,6 +1299,7 @@ __acquires(bitmap->lock)
 
 int bitmap_startwrite(struct bitmap *bitmap, sector_t offset, unsigned long sectors, int behind)
 {
+	dm_enter
 	if (!bitmap)
 		return 0;
 
@@ -1329,6 +1365,7 @@ EXPORT_SYMBOL(bitmap_startwrite);
 void bitmap_endwrite(struct bitmap *bitmap, sector_t offset, unsigned long sectors,
 		     int success, int behind)
 {
+	dm_enter
 	if (!bitmap)
 		return;
 	if (behind) {
@@ -1384,6 +1421,7 @@ static int __bitmap_start_sync(struct bi
 {
 	bitmap_counter_t *bmc;
 	int rv;
+	dm_enter
 	if (bitmap == NULL) {/* FIXME or bitmap set as 'failed' */
 		*blocks = 1024;
 		return 1; /* always resync if no bitmap */
@@ -1420,6 +1458,7 @@ int bitmap_start_sync(struct bitmap *bit
 	int rv = 0;
 	sector_t blocks1;
 
+	dm_enter
 	*blocks = 0;
 	while (*blocks < (PAGE_SIZE>>9)) {
 		rv |= __bitmap_start_sync(bitmap, offset,
@@ -1436,6 +1475,7 @@ void bitmap_end_sync(struct bitmap *bitm
 	bitmap_counter_t *bmc;
 	unsigned long flags;
 
+	dm_enter
 	if (bitmap == NULL) {
 		*blocks = 1024;
 		return;
@@ -1470,6 +1510,7 @@ void bitmap_close_sync(struct bitmap *bi
 	 */
 	sector_t sector = 0;
 	sector_t blocks;
+	dm_enter
 	if (!bitmap)
 		return;
 	while (sector < bitmap->mddev->resync_max_sectors) {
@@ -1484,6 +1525,7 @@ void bitmap_cond_end_sync(struct bitmap
 	sector_t s = 0;
 	sector_t blocks;
 
+	dm_enter
 	if (!bitmap)
 		return;
 	if (sector == 0) {
@@ -1518,6 +1560,7 @@ static void bitmap_set_memory_bits(struc
 
 	sector_t secs;
 	bitmap_counter_t *bmc;
+	dm_enter
 	spin_lock_irq(&bitmap->counts.lock);
 	bmc = bitmap_get_counter(&bitmap->counts, offset, &secs, 1);
 	if (!bmc) {
@@ -1538,6 +1581,7 @@ void bitmap_dirty_bits(struct bitmap *bi
 {
 	unsigned long chunk;
 
+	dm_enter
 	for (chunk = s; chunk <= e; chunk++) {
 		sector_t sec = (sector_t)chunk << bitmap->counts.chunkshift;
 		bitmap_set_memory_bits(bitmap, sec, 1);
@@ -1559,6 +1603,7 @@ void bitmap_flush(struct mddev *mddev)
 	struct bitmap *bitmap = mddev->bitmap;
 	long sleep;
 
+	dm_enter
 	if (!bitmap) /* there was no bitmap */
 		return;
 
@@ -1583,6 +1628,7 @@ static void bitmap_free(struct bitmap *b
 	unsigned long k, pages;
 	struct bitmap_page *bp;
 
+	dm_enter
 	if (!bitmap) /* there was no bitmap */
 		return;
 
@@ -1610,6 +1656,7 @@ void bitmap_destroy(struct mddev *mddev)
 {
 	struct bitmap *bitmap = mddev->bitmap;
 
+	dm_enter
 	if (!bitmap) /* there was no bitmap */
 		return;
 
@@ -1637,6 +1684,7 @@ int bitmap_create(struct mddev *mddev)
 	int err;
 	struct sysfs_dirent *bm = NULL;
 
+	dm_enter
 	BUILD_BUG_ON(sizeof(bitmap_super_t) != 256);
 
 	BUG_ON(file && mddev->bitmap_info.offset);
@@ -1714,6 +1762,7 @@ int bitmap_load(struct mddev *mddev)
 	sector_t sector = 0;
 	struct bitmap *bitmap = mddev->bitmap;
 
+	dm_enter
 	if (!bitmap)
 		goto out;
 
@@ -1763,6 +1812,7 @@ void bitmap_status(struct seq_file *seq,
 	unsigned long chunk_kb;
 	struct bitmap_counts *counts;
 
+	dm_enter
 	if (!bitmap)
 		return;
 
@@ -1808,6 +1858,7 @@ int bitmap_resize(struct bitmap *bitmap,
 	long pages;
 	struct bitmap_page *new_bp;
 
+	dm_enter
 	if (chunksize == 0) {
 		/* If there is enough space, leave the chunk size unchanged,
 		 * else increase by factor of two until there is enough space.
@@ -1953,6 +2004,7 @@ static ssize_t
 location_show(struct mddev *mddev, char *page)
 {
 	ssize_t len;
+	dm_enter
 	if (mddev->bitmap_info.file)
 		len = sprintf(page, "file");
 	else if (mddev->bitmap_info.offset)
@@ -1967,6 +2019,7 @@ static ssize_t
 location_store(struct mddev *mddev, const char *buf, size_t len)
 {
 
+	dm_enter
 	if (mddev->pers) {
 		if (!mddev->pers->quiesce)
 			return -EBUSY;
@@ -2049,6 +2102,7 @@ __ATTR(location, S_IRUGO|S_IWUSR, locati
 static ssize_t
 space_show(struct mddev *mddev, char *page)
 {
+	dm_enter
 	return sprintf(page, "%lu\n", mddev->bitmap_info.space);
 }
 
@@ -2058,6 +2112,7 @@ space_store(struct mddev *mddev, const c
 	unsigned long sectors;
 	int rv;
 
+	dm_enter
 	rv = kstrtoul(buf, 10, &sectors);
 	if (rv)
 		return rv;
@@ -2085,7 +2140,7 @@ timeout_show(struct mddev *mddev, char *
 	ssize_t len;
 	unsigned long secs = mddev->bitmap_info.daemon_sleep / HZ;
 	unsigned long jifs = mddev->bitmap_info.daemon_sleep % HZ;
-
+	dm_enter
 	len = sprintf(page, "%lu", secs);
 	if (jifs)
 		len += sprintf(page+len, ".%03u", jiffies_to_msecs(jifs));
@@ -2098,7 +2153,9 @@ timeout_store(struct mddev *mddev, const
 {
 	/* timeout can be set at any time */
 	unsigned long timeout;
-	int rv = strict_strtoul_scaled(buf, &timeout, 4);
+	int rv;
+	dm_enter
+       	rv = strict_strtoul_scaled(buf, &timeout, 4);
 	if (rv)
 		return rv;
 
@@ -2132,6 +2189,7 @@ __ATTR(time_base, S_IRUGO|S_IWUSR, timeo
 static ssize_t
 backlog_show(struct mddev *mddev, char *page)
 {
+	dm_enter
 	return sprintf(page, "%lu\n", mddev->bitmap_info.max_write_behind);
 }
 
@@ -2139,7 +2197,9 @@ static ssize_t
 backlog_store(struct mddev *mddev, const char *buf, size_t len)
 {
 	unsigned long backlog;
-	int rv = strict_strtoul(buf, 10, &backlog);
+	int rv;
+	dm_enter
+       	rv = strict_strtoul(buf, 10, &backlog);
 	if (rv)
 		return rv;
 	if (backlog > COUNTER_MAX)
@@ -2154,6 +2214,7 @@ __ATTR(backlog, S_IRUGO|S_IWUSR, backlog
 static ssize_t
 chunksize_show(struct mddev *mddev, char *page)
 {
+	dm_enter
 	return sprintf(page, "%lu\n", mddev->bitmap_info.chunksize);
 }
 
@@ -2163,6 +2224,7 @@ chunksize_store(struct mddev *mddev, con
 	/* Can only be changed when no bitmap is active */
 	int rv;
 	unsigned long csize;
+	dm_enter
 	if (mddev->bitmap)
 		return -EBUSY;
 	rv = strict_strtoul(buf, 10, &csize);
@@ -2180,12 +2242,14 @@ __ATTR(chunksize, S_IRUGO|S_IWUSR, chunk
 
 static ssize_t metadata_show(struct mddev *mddev, char *page)
 {
+	dm_enter
 	return sprintf(page, "%s\n", (mddev->bitmap_info.external
 				      ? "external" : "internal"));
 }
 
 static ssize_t metadata_store(struct mddev *mddev, const char *buf, size_t len)
 {
+	dm_enter
 	if (mddev->bitmap ||
 	    mddev->bitmap_info.file ||
 	    mddev->bitmap_info.offset)
@@ -2205,6 +2269,7 @@ __ATTR(metadata, S_IRUGO|S_IWUSR, metada
 static ssize_t can_clear_show(struct mddev *mddev, char *page)
 {
 	int len;
+	dm_enter
 	if (mddev->bitmap)
 		len = sprintf(page, "%s\n", (mddev->bitmap->need_sync ?
 					     "false" : "true"));
@@ -2215,6 +2280,7 @@ static ssize_t can_clear_show(struct mdd
 
 static ssize_t can_clear_store(struct mddev *mddev, const char *buf, size_t len)
 {
+	dm_enter
 	if (mddev->bitmap == NULL)
 		return -ENOENT;
 	if (strncmp(buf, "false", 5) == 0)
@@ -2234,6 +2300,7 @@ __ATTR(can_clear, S_IRUGO|S_IWUSR, can_c
 static ssize_t
 behind_writes_used_show(struct mddev *mddev, char *page)
 {
+	dm_enter
 	if (mddev->bitmap == NULL)
 		return sprintf(page, "0\n");
 	return sprintf(page, "%lu\n",
@@ -2243,6 +2310,7 @@ behind_writes_used_show(struct mddev *md
 static ssize_t
 behind_writes_used_reset(struct mddev *mddev, const char *buf, size_t len)
 {
+	dm_enter
 	if (mddev->bitmap)
 		mddev->bitmap->behind_writes_used = 0;
 	return len;
Index: linux-latency/drivers/md/dm-bufio.c
===================================================================
--- linux-latency.orig/drivers/md/dm-bufio.c
+++ linux-latency/drivers/md/dm-bufio.c
@@ -157,6 +157,7 @@ static char *dm_bufio_cache_names[PAGE_S
 static inline int dm_bufio_cache_index(struct dm_bufio_client *c)
 {
 	unsigned ret = c->blocks_per_page_bits - 1;
+	dm_enter
 
 	BUG_ON(ret >= ARRAY_SIZE(dm_bufio_caches));
 
@@ -170,16 +171,19 @@ static inline int dm_bufio_cache_index(s
 
 static void dm_bufio_lock(struct dm_bufio_client *c)
 {
+	dm_enter
 	mutex_lock_nested(&c->lock, dm_bufio_in_request());
 }
 
 static int dm_bufio_trylock(struct dm_bufio_client *c)
 {
+	dm_enter
 	return mutex_trylock(&c->lock);
 }
 
 static void dm_bufio_unlock(struct dm_bufio_client *c)
 {
+	dm_enter
 	mutex_unlock(&c->lock);
 }
 
@@ -260,6 +264,7 @@ static void adjust_total_allocated(enum
 		&dm_bufio_allocated_vmalloc,
 	};
 
+	dm_enter
 	spin_lock(&param_spinlock);
 
 	*class_ptr[data_mode] += diff;
@@ -277,6 +282,7 @@ static void adjust_total_allocated(enum
  */
 static void __cache_size_refresh(void)
 {
+	dm_enter
 	BUG_ON(!mutex_is_locked(&dm_bufio_clients_lock));
 	BUG_ON(dm_bufio_client_count < 0);
 
@@ -322,6 +328,7 @@ static void *alloc_buffer_data(struct dm
 	unsigned noio_flag;
 	void *ptr;
 
+	dm_enter
 	if (c->block_size <= DM_BUFIO_BLOCK_SIZE_SLAB_LIMIT) {
 		*data_mode = DATA_MODE_SLAB;
 		return kmem_cache_alloc(DM_BUFIO_CACHE(c), gfp_mask);
@@ -363,6 +370,7 @@ static void *alloc_buffer_data(struct dm
 static void free_buffer_data(struct dm_bufio_client *c,
 			     void *data, enum data_mode data_mode)
 {
+	dm_enter
 	switch (data_mode) {
 	case DATA_MODE_SLAB:
 		kmem_cache_free(DM_BUFIO_CACHE(c), data);
@@ -391,6 +399,7 @@ static struct dm_buffer *alloc_buffer(st
 	struct dm_buffer *b = kmalloc(sizeof(struct dm_buffer) + c->aux_size,
 				      gfp_mask);
 
+	dm_enter
 	if (!b)
 		return NULL;
 
@@ -414,6 +423,7 @@ static void free_buffer(struct dm_buffer
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	adjust_total_allocated(b->data_mode, -(long)c->block_size);
 
 	free_buffer_data(c, b->data, b->data_mode);
@@ -427,6 +437,7 @@ static void __link_buffer(struct dm_buff
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	c->n_buffers[dirty]++;
 	b->block = block;
 	b->list_mode = dirty;
@@ -442,6 +453,7 @@ static void __unlink_buffer(struct dm_bu
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	BUG_ON(!c->n_buffers[b->list_mode]);
 
 	c->n_buffers[b->list_mode]--;
@@ -456,6 +468,7 @@ static void __relink_lru(struct dm_buffe
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	BUG_ON(!c->n_buffers[b->list_mode]);
 
 	c->n_buffers[b->list_mode]--;
@@ -494,6 +507,7 @@ static void dmio_complete(unsigned long
 {
 	struct dm_buffer *b = context;
 
+	dm_enter
 	b->bio.bi_end_io(&b->bio, error ? -EIO : 0);
 }
 
@@ -512,6 +526,7 @@ static void use_dmio(struct dm_buffer *b
 		.sector = block << b->c->sectors_per_block_bits,
 		.count = b->c->block_size >> SECTOR_SHIFT,
 	};
+	dm_enter
 
 	if (b->data_mode != DATA_MODE_VMALLOC) {
 		io_req.mem.type = DM_IO_KMEM;
@@ -534,6 +549,7 @@ static void use_inline_bio(struct dm_buf
 	char *ptr;
 	int len;
 
+	dm_enter
 	bio_init(&b->bio);
 	b->bio.bi_io_vec = b->bio_vec;
 	b->bio.bi_max_vecs = DM_BUFIO_INLINE_VECS;
@@ -572,6 +588,7 @@ static void use_inline_bio(struct dm_buf
 static void submit_io(struct dm_buffer *b, int rw, sector_t block,
 		      bio_end_io_t *end_io)
 {
+	dm_enter
 	if (rw == WRITE && b->c->write_callback)
 		b->c->write_callback(b);
 
@@ -596,6 +613,7 @@ static void write_endio(struct bio *bio,
 {
 	struct dm_buffer *b = container_of(bio, struct dm_buffer, bio);
 
+	dm_enter
 	b->write_error = error;
 	if (unlikely(error)) {
 		struct dm_bufio_client *c = b->c;
@@ -616,6 +634,7 @@ static void write_endio(struct bio *bio,
  */
 static int do_io_schedule(void *word)
 {
+	dm_enter
 	io_schedule();
 
 	return 0;
@@ -632,6 +651,7 @@ static int do_io_schedule(void *word)
  */
 static void __write_dirty_buffer(struct dm_buffer *b)
 {
+	dm_enter
 	if (!test_bit(B_DIRTY, &b->state))
 		return;
 
@@ -649,6 +669,7 @@ static void __write_dirty_buffer(struct
  */
 static void __make_buffer_clean(struct dm_buffer *b)
 {
+	dm_enter
 	BUG_ON(b->hold_count);
 
 	if (!b->state)	/* fast case */
@@ -667,6 +688,7 @@ static struct dm_buffer *__get_unclaimed
 {
 	struct dm_buffer *b;
 
+	dm_enter
 	list_for_each_entry_reverse(b, &c->lru[LIST_CLEAN], lru_list) {
 		BUG_ON(test_bit(B_WRITING, &b->state));
 		BUG_ON(test_bit(B_DIRTY, &b->state));
@@ -703,6 +725,7 @@ static struct dm_buffer *__get_unclaimed
 static void __wait_for_free_buffer(struct dm_bufio_client *c)
 {
 	DECLARE_WAITQUEUE(wait, current);
+	dm_enter
 
 	add_wait_queue(&c->free_buffer_wait, &wait);
 	set_task_state(current, TASK_UNINTERRUPTIBLE);
@@ -733,6 +756,7 @@ static struct dm_buffer *__alloc_buffer_
 {
 	struct dm_buffer *b;
 
+	dm_enter
 	/*
 	 * dm-bufio is resistant to allocation failures (it just keeps
 	 * one buffer reserved in cases all the allocations fail).
@@ -774,7 +798,9 @@ static struct dm_buffer *__alloc_buffer_
 
 static struct dm_buffer *__alloc_buffer_wait(struct dm_bufio_client *c, enum new_flag nf)
 {
-	struct dm_buffer *b = __alloc_buffer_wait_no_callback(c, nf);
+	struct dm_buffer *b;
+	dm_enter
+       	b = __alloc_buffer_wait_no_callback(c, nf);
 
 	if (!b)
 		return NULL;
@@ -792,6 +818,7 @@ static void __free_buffer_wake(struct dm
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	if (!c->need_reserved_buffers)
 		free_buffer(b);
 	else {
@@ -806,6 +833,7 @@ static void __write_dirty_buffers_async(
 {
 	struct dm_buffer *b, *tmp;
 
+	dm_enter
 	list_for_each_entry_safe_reverse(b, tmp, &c->lru[LIST_DIRTY], lru_list) {
 		BUG_ON(test_bit(B_READING, &b->state));
 
@@ -832,6 +860,7 @@ static void __get_memory_limit(struct dm
 {
 	unsigned long buffers;
 
+	dm_enter
 	if (ACCESS_ONCE(dm_bufio_cache_size) != dm_bufio_cache_size_latch) {
 		mutex_lock(&dm_bufio_clients_lock);
 		__cache_size_refresh();
@@ -857,6 +886,7 @@ static void __check_watermark(struct dm_
 {
 	unsigned long threshold_buffers, limit_buffers;
 
+	dm_enter
 	__get_memory_limit(c, &threshold_buffers, &limit_buffers);
 
 	while (c->n_buffers[LIST_CLEAN] + c->n_buffers[LIST_DIRTY] >
@@ -882,6 +912,7 @@ static struct dm_buffer *__find(struct d
 {
 	struct dm_buffer *b;
 
+	dm_enter
 	hlist_for_each_entry(b, &c->cache_hash[DM_BUFIO_HASH(block)],
 			     hash_list) {
 		dm_bufio_cond_resched();
@@ -901,6 +932,7 @@ static struct dm_buffer *__bufio_new(str
 {
 	struct dm_buffer *b, *new_b = NULL;
 
+	dm_enter
 	*need_submit = 0;
 
 	b = __find(c, block);
@@ -969,6 +1001,7 @@ static void read_endio(struct bio *bio,
 {
 	struct dm_buffer *b = container_of(bio, struct dm_buffer, bio);
 
+	dm_enter
 	b->read_error = error;
 
 	BUG_ON(!test_bit(B_READING, &b->state));
@@ -992,6 +1025,7 @@ static void *new_read(struct dm_bufio_cl
 	int need_submit;
 	struct dm_buffer *b;
 
+	dm_enter
 	dm_bufio_lock(c);
 	b = __bufio_new(c, block, nf, &need_submit);
 	dm_bufio_unlock(c);
@@ -1020,6 +1054,7 @@ static void *new_read(struct dm_bufio_cl
 void *dm_bufio_get(struct dm_bufio_client *c, sector_t block,
 		   struct dm_buffer **bp)
 {
+	dm_enter
 	return new_read(c, block, NF_GET, bp);
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get);
@@ -1027,6 +1062,7 @@ EXPORT_SYMBOL_GPL(dm_bufio_get);
 void *dm_bufio_read(struct dm_bufio_client *c, sector_t block,
 		    struct dm_buffer **bp)
 {
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	return new_read(c, block, NF_READ, bp);
@@ -1036,6 +1072,7 @@ EXPORT_SYMBOL_GPL(dm_bufio_read);
 void *dm_bufio_new(struct dm_bufio_client *c, sector_t block,
 		   struct dm_buffer **bp)
 {
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	return new_read(c, block, NF_FRESH, bp);
@@ -1049,6 +1086,7 @@ void dm_bufio_prefetch(struct dm_bufio_c
 
 	BUG_ON(dm_bufio_in_request());
 
+	dm_enter
 	blk_start_plug(&plug);
 	dm_bufio_lock(c);
 
@@ -1083,6 +1121,7 @@ void dm_bufio_release(struct dm_buffer *
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	dm_bufio_lock(c);
 
 	BUG_ON(!b->hold_count);
@@ -1113,6 +1152,7 @@ void dm_bufio_mark_buffer_dirty(struct d
 {
 	struct dm_bufio_client *c = b->c;
 
+	dm_enter
 	dm_bufio_lock(c);
 
 	BUG_ON(test_bit(B_READING, &b->state));
@@ -1126,6 +1166,7 @@ EXPORT_SYMBOL_GPL(dm_bufio_mark_buffer_d
 
 void dm_bufio_write_dirty_buffers_async(struct dm_bufio_client *c)
 {
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	dm_bufio_lock(c);
@@ -1147,6 +1188,7 @@ int dm_bufio_write_dirty_buffers(struct
 	unsigned long buffers_processed = 0;
 	struct dm_buffer *b, *tmp;
 
+	dm_enter
 	dm_bufio_lock(c);
 	__write_dirty_buffers_async(c, 0);
 
@@ -1227,6 +1269,7 @@ int dm_bufio_issue_flush(struct dm_bufio
 		.count = 0,
 	};
 
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	return dm_io(&io_req, 1, &io_reg, NULL);
@@ -1250,6 +1293,7 @@ void dm_bufio_release_move(struct dm_buf
 	struct dm_bufio_client *c = b->c;
 	struct dm_buffer *new;
 
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	dm_bufio_lock(c);
@@ -1309,12 +1353,14 @@ EXPORT_SYMBOL_GPL(dm_bufio_release_move)
 
 unsigned dm_bufio_get_block_size(struct dm_bufio_client *c)
 {
+	dm_enter
 	return c->block_size;
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get_block_size);
 
 sector_t dm_bufio_get_device_size(struct dm_bufio_client *c)
 {
+	dm_enter
 	return i_size_read(c->bdev->bd_inode) >>
 			   (SECTOR_SHIFT + c->sectors_per_block_bits);
 }
@@ -1322,24 +1368,28 @@ EXPORT_SYMBOL_GPL(dm_bufio_get_device_si
 
 sector_t dm_bufio_get_block_number(struct dm_buffer *b)
 {
+	dm_enter
 	return b->block;
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get_block_number);
 
 void *dm_bufio_get_block_data(struct dm_buffer *b)
 {
+	dm_enter
 	return b->data;
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get_block_data);
 
 void *dm_bufio_get_aux_data(struct dm_buffer *b)
 {
+	dm_enter
 	return b + 1;
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get_aux_data);
 
 struct dm_bufio_client *dm_bufio_get_client(struct dm_buffer *b)
 {
+	dm_enter
 	return b->c;
 }
 EXPORT_SYMBOL_GPL(dm_bufio_get_client);
@@ -1349,6 +1399,7 @@ static void drop_buffers(struct dm_bufio
 	struct dm_buffer *b;
 	int i;
 
+	dm_enter
 	BUG_ON(dm_bufio_in_request());
 
 	/*
@@ -1381,6 +1432,7 @@ static void drop_buffers(struct dm_bufio
 static int __cleanup_old_buffer(struct dm_buffer *b, gfp_t gfp,
 				unsigned long max_jiffies)
 {
+	dm_enter
 	if (jiffies - b->last_accessed < max_jiffies)
 		return 1;
 
@@ -1407,6 +1459,7 @@ static void __scan(struct dm_bufio_clien
 	int l;
 	struct dm_buffer *b, *tmp;
 
+	dm_enter
 	for (l = 0; l < LIST_SIZE; l++) {
 		list_for_each_entry_safe_reverse(b, tmp, &c->lru[l], lru_list)
 			if (!__cleanup_old_buffer(b, sc->gfp_mask, 0) &&
@@ -1423,6 +1476,7 @@ static int shrink(struct shrinker *shrin
 	unsigned long r;
 	unsigned long nr_to_scan = sc->nr_to_scan;
 
+	dm_enter
 	if (sc->gfp_mask & __GFP_IO)
 		dm_bufio_lock(c);
 	else if (!dm_bufio_trylock(c))
@@ -1452,6 +1506,7 @@ struct dm_bufio_client *dm_bufio_client_
 	struct dm_bufio_client *c;
 	unsigned i;
 
+	dm_enter
 	BUG_ON(block_size < 1 << SECTOR_SHIFT ||
 	       (block_size & (block_size - 1)));
 
@@ -1572,6 +1627,7 @@ void dm_bufio_client_destroy(struct dm_b
 {
 	unsigned i;
 
+	dm_enter
 	drop_buffers(c);
 
 	unregister_shrinker(&c->shrinker);
@@ -1614,6 +1670,7 @@ static void cleanup_old_buffers(void)
 	unsigned long max_age = ACCESS_ONCE(dm_bufio_max_age);
 	struct dm_bufio_client *c;
 
+	dm_enter
 	if (max_age > ULONG_MAX / HZ)
 		max_age = ULONG_MAX / HZ;
 
@@ -1642,6 +1699,7 @@ static struct delayed_work dm_bufio_work
 
 static void work_fn(struct work_struct *w)
 {
+	dm_enter
 	cleanup_old_buffers();
 
 	queue_delayed_work(dm_bufio_wq, &dm_bufio_work,
@@ -1660,6 +1718,7 @@ static int __init dm_bufio_init(void)
 {
 	__u64 mem;
 
+	dm_enter
 	memset(&dm_bufio_caches, 0, sizeof dm_bufio_caches);
 	memset(&dm_bufio_cache_names, 0, sizeof dm_bufio_cache_names);
 
@@ -1703,6 +1762,7 @@ static void __exit dm_bufio_exit(void)
 	int bug = 0;
 	int i;
 
+	dm_enter
 	cancel_delayed_work_sync(&dm_bufio_work);
 	destroy_workqueue(dm_bufio_wq);
 
Index: linux-latency/drivers/md/dm-io.c
===================================================================
--- linux-latency.orig/drivers/md/dm-io.c
+++ linux-latency/drivers/md/dm-io.c
@@ -51,6 +51,7 @@ struct dm_io_client *dm_io_client_create
 {
 	struct dm_io_client *client;
 
+	dm_enter
 	client = kmalloc(sizeof(*client), GFP_KERNEL);
 	if (!client)
 		return ERR_PTR(-ENOMEM);
@@ -75,6 +76,7 @@ EXPORT_SYMBOL(dm_io_client_create);
 
 void dm_io_client_destroy(struct dm_io_client *client)
 {
+	dm_enter
 	mempool_destroy(client->pool);
 	bioset_free(client->bios);
 	kfree(client);
@@ -91,6 +93,7 @@ EXPORT_SYMBOL(dm_io_client_destroy);
 static void store_io_and_region_in_bio(struct bio *bio, struct io *io,
 				       unsigned region)
 {
+	dm_enter
 	if (unlikely(!IS_ALIGNED((unsigned long)io, DM_IO_MAX_REGIONS))) {
 		DMCRIT("Unaligned struct io pointer %p", io);
 		BUG();
@@ -104,6 +107,7 @@ static void retrieve_io_and_region_from_
 {
 	unsigned long val = (unsigned long)bio->bi_private;
 
+	dm_enter
 	*io = (void *)(val & -(unsigned long)DM_IO_MAX_REGIONS);
 	*region = val & (DM_IO_MAX_REGIONS - 1);
 }
@@ -114,6 +118,7 @@ static void retrieve_io_and_region_from_
  *---------------------------------------------------------------*/
 static void dec_count(struct io *io, unsigned int region, int error)
 {
+	dm_enter
 	if (error)
 		set_bit(region, &io->error_bits);
 
@@ -141,6 +146,7 @@ static void endio(struct bio *bio, int e
 	struct io *io;
 	unsigned region;
 
+	dm_enter
 	if (error && bio_data_dir(bio) == READ)
 		zero_fill_bio(bio);
 
@@ -179,6 +185,7 @@ static void list_get_page(struct dpages
 	unsigned o = dp->context_u;
 	struct page_list *pl = (struct page_list *) dp->context_ptr;
 
+	dm_enter
 	*p = pl->page;
 	*len = PAGE_SIZE - o;
 	*offset = o;
@@ -187,12 +194,14 @@ static void list_get_page(struct dpages
 static void list_next_page(struct dpages *dp)
 {
 	struct page_list *pl = (struct page_list *) dp->context_ptr;
+	dm_enter
 	dp->context_ptr = pl->next;
 	dp->context_u = 0;
 }
 
 static void list_dp_init(struct dpages *dp, struct page_list *pl, unsigned offset)
 {
+	dm_enter
 	dp->get_page = list_get_page;
 	dp->next_page = list_next_page;
 	dp->context_u = offset;
@@ -206,6 +215,7 @@ static void bvec_get_page(struct dpages
 		  struct page **p, unsigned long *len, unsigned *offset)
 {
 	struct bio_vec *bvec = (struct bio_vec *) dp->context_ptr;
+	dm_enter
 	*p = bvec->bv_page;
 	*len = bvec->bv_len;
 	*offset = bvec->bv_offset;
@@ -214,11 +224,13 @@ static void bvec_get_page(struct dpages
 static void bvec_next_page(struct dpages *dp)
 {
 	struct bio_vec *bvec = (struct bio_vec *) dp->context_ptr;
+	dm_enter
 	dp->context_ptr = bvec + 1;
 }
 
 static void bvec_dp_init(struct dpages *dp, struct bio_vec *bvec)
 {
+	dm_enter
 	dp->get_page = bvec_get_page;
 	dp->next_page = bvec_next_page;
 	dp->context_ptr = bvec;
@@ -230,6 +242,7 @@ static void bvec_dp_init(struct dpages *
 static void vm_get_page(struct dpages *dp,
 		 struct page **p, unsigned long *len, unsigned *offset)
 {
+	dm_enter
 	*p = vmalloc_to_page(dp->context_ptr);
 	*offset = dp->context_u;
 	*len = PAGE_SIZE - dp->context_u;
@@ -237,12 +250,14 @@ static void vm_get_page(struct dpages *d
 
 static void vm_next_page(struct dpages *dp)
 {
+	dm_enter
 	dp->context_ptr += PAGE_SIZE - dp->context_u;
 	dp->context_u = 0;
 }
 
 static void vm_dp_init(struct dpages *dp, void *data)
 {
+	dm_enter
 	dp->get_page = vm_get_page;
 	dp->next_page = vm_next_page;
 	dp->context_u = ((unsigned long) data) & (PAGE_SIZE - 1);
@@ -255,6 +270,7 @@ static void vm_dp_init(struct dpages *dp
 static void km_get_page(struct dpages *dp, struct page **p, unsigned long *len,
 			unsigned *offset)
 {
+	dm_enter
 	*p = virt_to_page(dp->context_ptr);
 	*offset = dp->context_u;
 	*len = PAGE_SIZE - dp->context_u;
@@ -262,12 +278,14 @@ static void km_get_page(struct dpages *d
 
 static void km_next_page(struct dpages *dp)
 {
+	dm_enter
 	dp->context_ptr += PAGE_SIZE - dp->context_u;
 	dp->context_u = 0;
 }
 
 static void km_dp_init(struct dpages *dp, void *data)
 {
+	dm_enter
 	dp->get_page = km_get_page;
 	dp->next_page = km_next_page;
 	dp->context_u = ((unsigned long) data) & (PAGE_SIZE - 1);
@@ -290,6 +308,7 @@ static void do_region(int rw, unsigned r
 	unsigned short logical_block_size = queue_logical_block_size(q);
 	sector_t num_sectors;
 
+	dm_enter
 	/*
 	 * where->count may be zero if rw holds a flush and we need to
 	 * send a zero-sized flush.
@@ -352,6 +371,7 @@ static void dispatch_io(int rw, unsigned
 	int i;
 	struct dpages old_pages = *dp;
 
+	dm_enter
 	BUG_ON(num_regions > DM_IO_MAX_REGIONS);
 
 	if (sync)
@@ -386,6 +406,7 @@ static int sync_io(struct dm_io_client *
 	 */
 	volatile char io_[sizeof(struct io) + __alignof__(struct io) - 1];
 	struct io *io = (struct io *)PTR_ALIGN(&io_, __alignof__(struct io));
+	dm_enter
 
 	if (num_regions > 1 && (rw & RW_MASK) != WRITE) {
 		WARN_ON(1);
@@ -424,6 +445,7 @@ static int async_io(struct dm_io_client
 {
 	struct io *io;
 
+	dm_enter
 	if (num_regions > 1 && (rw & RW_MASK) != WRITE) {
 		WARN_ON(1);
 		fn(1, context);
@@ -448,6 +470,7 @@ static int async_io(struct dm_io_client
 static int dp_init(struct dm_io_request *io_req, struct dpages *dp,
 		   unsigned long size)
 {
+	dm_enter
 	/* Set up dpages based on memory type */
 
 	dp->vma_invalidate_address = NULL;
@@ -496,6 +519,7 @@ int dm_io(struct dm_io_request *io_req,
 	int r;
 	struct dpages dp;
 
+	dm_enter
 	r = dp_init(io_req, &dp, (unsigned long)where->count << SECTOR_SHIFT);
 	if (r)
 		return r;
@@ -511,6 +535,7 @@ EXPORT_SYMBOL(dm_io);
 
 int __init dm_io_init(void)
 {
+	dm_enter
 	_dm_io_cache = KMEM_CACHE(io, 0);
 	if (!_dm_io_cache)
 		return -ENOMEM;
@@ -520,6 +545,7 @@ int __init dm_io_init(void)
 
 void dm_io_exit(void)
 {
+	dm_enter
 	kmem_cache_destroy(_dm_io_cache);
 	_dm_io_cache = NULL;
 }
Index: linux-latency/drivers/md/dm-ioctl.c
===================================================================
--- linux-latency.orig/drivers/md/dm-ioctl.c
+++ linux-latency/drivers/md/dm-ioctl.c
@@ -65,12 +65,14 @@ static void init_buckets(struct list_hea
 {
 	unsigned int i;
 
+	dm_enter
 	for (i = 0; i < NUM_BUCKETS; i++)
 		INIT_LIST_HEAD(buckets + i);
 }
 
 static int dm_hash_init(void)
 {
+	dm_enter
 	init_buckets(_name_buckets);
 	init_buckets(_uuid_buckets);
 	return 0;
@@ -78,6 +80,7 @@ static int dm_hash_init(void)
 
 static void dm_hash_exit(void)
 {
+	dm_enter
 	dm_hash_remove_all(0);
 }
 
@@ -91,6 +94,7 @@ static unsigned int hash_str(const char
 	const unsigned int hash_mult = 2654435387U;
 	unsigned int h = 0;
 
+	dm_enter
 	while (*str)
 		h = (h + (unsigned int) *str++) * hash_mult;
 
@@ -105,6 +109,7 @@ static struct hash_cell *__get_name_cell
 	struct hash_cell *hc;
 	unsigned int h = hash_str(str);
 
+	dm_enter
 	list_for_each_entry (hc, _name_buckets + h, name_list)
 		if (!strcmp(hc->name, str)) {
 			dm_get(hc->md);
@@ -119,6 +124,7 @@ static struct hash_cell *__get_uuid_cell
 	struct hash_cell *hc;
 	unsigned int h = hash_str(str);
 
+	dm_enter
 	list_for_each_entry (hc, _uuid_buckets + h, uuid_list)
 		if (!strcmp(hc->uuid, str)) {
 			dm_get(hc->md);
@@ -133,6 +139,7 @@ static struct hash_cell *__get_dev_cell(
 	struct mapped_device *md;
 	struct hash_cell *hc;
 
+	dm_enter
 	md = dm_get_md(huge_decode_dev(dev));
 	if (!md)
 		return NULL;
@@ -154,6 +161,7 @@ static struct hash_cell *alloc_cell(cons
 {
 	struct hash_cell *hc;
 
+	dm_enter
 	hc = kmalloc(sizeof(*hc), GFP_KERNEL);
 	if (!hc)
 		return NULL;
@@ -185,6 +193,7 @@ static struct hash_cell *alloc_cell(cons
 
 static void free_cell(struct hash_cell *hc)
 {
+	dm_enter
 	if (hc) {
 		kfree(hc->name);
 		kfree(hc->uuid);
@@ -200,6 +209,7 @@ static int dm_hash_insert(const char *na
 {
 	struct hash_cell *cell, *hc;
 
+	dm_enter
 	/*
 	 * Allocate the new cells.
 	 */
@@ -246,6 +256,7 @@ static void __hash_remove(struct hash_ce
 {
 	struct dm_table *table;
 
+	dm_enter
 	/* remove from the dev hash */
 	list_del(&hc->uuid_list);
 	list_del(&hc->name_list);
@@ -271,6 +282,7 @@ static void dm_hash_remove_all(int keep_
 	struct hash_cell *hc;
 	struct mapped_device *md;
 
+	dm_enter
 retry:
 	dev_skipped = 0;
 
@@ -318,6 +330,7 @@ retry:
  */
 static void __set_cell_uuid(struct hash_cell *hc, char *new_uuid)
 {
+	dm_enter
 	mutex_lock(&dm_hash_cells_mutex);
 	hc->uuid = new_uuid;
 	mutex_unlock(&dm_hash_cells_mutex);
@@ -333,6 +346,7 @@ static char *__change_cell_name(struct h
 {
 	char *old_name;
 
+	dm_enter
 	/*
 	 * Rename and move the name cell.
 	 */
@@ -357,6 +371,7 @@ static struct mapped_device *dm_hash_ren
 	struct mapped_device *md;
 	unsigned change_uuid = (param->flags & DM_UUID_FLAG) ? 1 : 0;
 
+	dm_enter
 	/*
 	 * duplicate new.
 	 */
@@ -445,6 +460,7 @@ typedef int (*ioctl_fn)(struct dm_ioctl
 
 static int remove_all(struct dm_ioctl *param, size_t param_size)
 {
+	dm_enter
 	dm_hash_remove_all(1);
 	param->data_size = 0;
 	return 0;
@@ -456,6 +472,7 @@ static int remove_all(struct dm_ioctl *p
 #define ALIGN_MASK 7
 static inline void *align_ptr(void *ptr)
 {
+	dm_enter
 	return (void *) (((size_t) (ptr + ALIGN_MASK)) & ~ALIGN_MASK);
 }
 
@@ -466,6 +483,7 @@ static inline void *align_ptr(void *ptr)
 static void *get_result_buffer(struct dm_ioctl *param, size_t param_size,
 			       size_t *len)
 {
+	dm_enter
 	param->data_start = align_ptr(param + 1) - (void *) param;
 
 	if (param->data_start < param_size)
@@ -484,6 +502,7 @@ static int list_devices(struct dm_ioctl
 	struct gendisk *disk;
 	struct dm_name_list *nl, *old_nl = NULL;
 
+	dm_enter
 	down_write(&_hash_lock);
 
 	/*
@@ -537,6 +556,7 @@ static void list_version_get_needed(stru
 {
     size_t *needed = needed_param;
 
+    dm_enter
     *needed += sizeof(struct dm_target_versions);
     *needed += strlen(tt->name);
     *needed += ALIGN_MASK;
@@ -546,6 +566,7 @@ static void list_version_get_info(struct
 {
     struct vers_iter *info = param;
 
+    dm_enter
     /* Check space - it might have changed since the first iteration */
     if ((char *)info->vers + sizeof(tt->version) + strlen(tt->name) + 1 >
 	info->end) {
@@ -573,6 +594,7 @@ static int list_versions(struct dm_ioctl
 	struct dm_target_versions *vers;
 	struct vers_iter iter_info;
 
+	dm_enter
 	/*
 	 * Loop through all the devices working out how much
 	 * space we need.
@@ -607,6 +629,7 @@ static int list_versions(struct dm_ioctl
 
 static int check_name(const char *name)
 {
+	dm_enter
 	if (strchr(name, '/')) {
 		DMWARN("invalid device name");
 		return -EINVAL;
@@ -625,6 +648,7 @@ static struct dm_table *dm_get_inactive_
 	struct hash_cell *hc;
 	struct dm_table *table = NULL;
 
+	dm_enter
 	down_read(&_hash_lock);
 	hc = dm_get_mdptr(md);
 	if (!hc || hc->md != md) {
@@ -658,6 +682,7 @@ static void __dev_status(struct mapped_d
 	struct gendisk *disk = dm_disk(md);
 	struct dm_table *table;
 
+	dm_enter
 	param->flags &= ~(DM_SUSPEND_FLAG | DM_READONLY_FLAG |
 			  DM_ACTIVE_PRESENT_FLAG);
 
@@ -704,6 +729,7 @@ static int dev_create(struct dm_ioctl *p
 	int r, m = DM_ANY_MINOR;
 	struct mapped_device *md;
 
+	dm_enter
 	r = check_name(param->name);
 	if (r)
 		return r;
@@ -738,6 +764,7 @@ static struct hash_cell *__find_device_h
 {
 	struct hash_cell *hc = NULL;
 
+	dm_enter
 	if (*param->uuid) {
 		if (*param->name || param->dev)
 			return NULL;
@@ -782,6 +809,7 @@ static struct mapped_device *find_device
 	struct hash_cell *hc;
 	struct mapped_device *md = NULL;
 
+	dm_enter
 	down_read(&_hash_lock);
 	hc = __find_device_hash_cell(param);
 	if (hc)
@@ -797,6 +825,7 @@ static int dev_remove(struct dm_ioctl *p
 	struct mapped_device *md;
 	int r;
 
+	dm_enter
 	down_write(&_hash_lock);
 	hc = __find_device_hash_cell(param);
 
@@ -836,6 +865,7 @@ static int dev_remove(struct dm_ioctl *p
  */
 static int invalid_str(char *str, void *end)
 {
+	dm_enter
 	while ((void *) str < end)
 		if (!*str++)
 			return 0;
@@ -850,6 +880,7 @@ static int dev_rename(struct dm_ioctl *p
 	struct mapped_device *md;
 	unsigned change_uuid = (param->flags & DM_UUID_FLAG) ? 1 : 0;
 
+	dm_enter
 	if (new_data < param->data ||
 	    invalid_str(new_data, (void *) param + param_size) ||
 	    strlen(new_data) > (change_uuid ? DM_UUID_LEN - 1 : DM_NAME_LEN - 1)) {
@@ -882,6 +913,7 @@ static int dev_set_geometry(struct dm_io
 	char *geostr = (char *) param + param->data_start;
 	char dummy;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -926,6 +958,7 @@ static int do_suspend(struct dm_ioctl *p
 	unsigned suspend_flags = DM_SUSPEND_LOCKFS_FLAG;
 	struct mapped_device *md;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -957,6 +990,7 @@ static int do_resume(struct dm_ioctl *pa
 	struct mapped_device *md;
 	struct dm_table *new_map, *old_map = NULL;
 
+	dm_enter
 	down_write(&_hash_lock);
 
 	hc = __find_device_hash_cell(param);
@@ -1019,6 +1053,7 @@ static int do_resume(struct dm_ioctl *pa
  */
 static int dev_suspend(struct dm_ioctl *param, size_t param_size)
 {
+	dm_enter
 	if (param->flags & DM_SUSPEND_FLAG)
 		return do_suspend(param);
 
@@ -1033,6 +1068,7 @@ static int dev_status(struct dm_ioctl *p
 {
 	struct mapped_device *md;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1056,6 +1092,7 @@ static void retrieve_status(struct dm_ta
 	size_t remaining, len, used = 0;
 	unsigned status_flags = 0;
 
+	dm_enter
 	outptr = outbuf = get_result_buffer(param, param_size, &len);
 
 	if (param->flags & DM_STATUS_TABLE_FLAG)
@@ -1126,6 +1163,7 @@ static int dev_wait(struct dm_ioctl *par
 	struct mapped_device *md;
 	struct dm_table *table;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1161,6 +1199,7 @@ static inline fmode_t get_mode(struct dm
 {
 	fmode_t mode = FMODE_READ | FMODE_WRITE;
 
+	dm_enter
 	if (param->flags & DM_READONLY_FLAG)
 		mode = FMODE_READ;
 
@@ -1173,6 +1212,7 @@ static int next_target(struct dm_target_
 	*spec = (struct dm_target_spec *) ((unsigned char *) last + next);
 	*target_params = (char *) (*spec + 1);
 
+	dm_enter
 	if (*spec < (last + 1))
 		return -EINVAL;
 
@@ -1189,6 +1229,7 @@ static int populate_table(struct dm_tabl
 	void *end = (void *) param + param_size;
 	char *target_params;
 
+	dm_enter
 	if (!param->target_count) {
 		DMWARN("populate_table: no targets specified");
 		return -EINVAL;
@@ -1225,6 +1266,7 @@ static int table_load(struct dm_ioctl *p
 	struct mapped_device *md;
 	struct target_type *immutable_target_type;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1302,6 +1344,7 @@ static int table_clear(struct dm_ioctl *
 	struct hash_cell *hc;
 	struct mapped_device *md;
 
+	dm_enter
 	down_write(&_hash_lock);
 
 	hc = __find_device_hash_cell(param);
@@ -1338,6 +1381,7 @@ static void retrieve_deps(struct dm_tabl
 	struct dm_dev_internal *dd;
 	struct dm_target_deps *deps;
 
+	dm_enter
 	deps = get_result_buffer(param, param_size, &len);
 
 	/*
@@ -1371,6 +1415,7 @@ static int table_deps(struct dm_ioctl *p
 	struct mapped_device *md;
 	struct dm_table *table;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1397,6 +1442,7 @@ static int table_status(struct dm_ioctl
 	struct mapped_device *md;
 	struct dm_table *table;
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1444,6 +1490,7 @@ static int target_message(struct dm_ioct
 	size_t maxlen;
 	char *result = get_result_buffer(param, param_size, &maxlen);
 
+	dm_enter
 	md = find_device(param);
 	if (!md)
 		return -ENXIO;
@@ -1568,6 +1615,7 @@ static int check_version(unsigned int cm
 	uint32_t version[3];
 	int r = 0;
 
+	dm_enter
 	if (copy_from_user(version, user->version, sizeof(version)))
 		return -EFAULT;
 
@@ -1599,6 +1647,7 @@ static int check_version(unsigned int cm
 
 static void free_params(struct dm_ioctl *param, size_t param_size, int param_flags)
 {
+	dm_enter
 	if (param_flags & DM_WIPE_BUFFER)
 		memset(param, 0, param_size);
 
@@ -1682,6 +1731,7 @@ bad:
 
 static int validate_params(uint cmd, struct dm_ioctl *param)
 {
+	dm_enter
 	/* Always clear this flag */
 	param->flags &= ~DM_BUFFER_FULL_FLAG;
 	param->flags &= ~DM_UEVENT_GENERATED_FLAG;
@@ -1722,6 +1772,7 @@ static int ctl_ioctl(uint command, struc
 	size_t input_param_size;
 	struct dm_ioctl param_kernel;
 
+	dm_enter
 	/* only root can play with this */
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
@@ -1790,6 +1841,7 @@ static long dm_ctl_ioctl(struct file *fi
 #ifdef CONFIG_COMPAT
 static long dm_compat_ctl_ioctl(struct file *file, uint command, ulong u)
 {
+	dm_enter
 	return (long)dm_ctl_ioctl(file, command, (ulong) compat_ptr(u));
 }
 #else
@@ -1821,6 +1873,7 @@ int __init dm_interface_init(void)
 {
 	int r;
 
+	dm_enter
 	r = dm_hash_init();
 	if (r)
 		return r;
@@ -1857,6 +1910,7 @@ int dm_copy_name_and_uuid(struct mapped_
 	int r = 0;
 	struct hash_cell *hc;
 
+	dm_enter
 	if (!md)
 		return -ENXIO;
 
Index: linux-latency/drivers/md/dm-kcopyd.c
===================================================================
--- linux-latency.orig/drivers/md/dm-kcopyd.c
+++ linux-latency/drivers/md/dm-kcopyd.c
@@ -184,6 +184,7 @@ skip_limit:
 
 static void wake(struct dm_kcopyd_client *kc)
 {
+	dm_enter
 	queue_work(kc->kcopyd_wq, &kc->kcopyd_work);
 }
 
@@ -193,6 +194,7 @@ static void wake(struct dm_kcopyd_client
 static struct page_list *alloc_pl(gfp_t gfp)
 {
 	struct page_list *pl;
+	dm_enter
 
 	pl = kmalloc(sizeof(*pl), gfp);
 	if (!pl)
@@ -209,6 +211,7 @@ static struct page_list *alloc_pl(gfp_t
 
 static void free_pl(struct page_list *pl)
 {
+	dm_enter
 	__free_page(pl->page);
 	kfree(pl);
 }
@@ -221,6 +224,7 @@ static void kcopyd_put_pages(struct dm_k
 {
 	struct page_list *next;
 
+	dm_enter
 	do {
 		next = pl->next;
 
@@ -241,6 +245,7 @@ static int kcopyd_get_pages(struct dm_kc
 {
 	struct page_list *pl;
 
+	dm_enter
 	*pages = NULL;
 
 	do {
@@ -272,6 +277,7 @@ static void drop_pages(struct page_list
 {
 	struct page_list *next;
 
+	dm_enter
 	while (pl) {
 		next = pl->next;
 		free_pl(pl);
@@ -287,6 +293,7 @@ static int client_reserve_pages(struct d
 	unsigned i;
 	struct page_list *pl = NULL, *next;
 
+	dm_enter
 	for (i = 0; i < nr_pages; i++) {
 		next = alloc_pl(GFP_KERNEL);
 		if (!next) {
@@ -306,6 +313,7 @@ static int client_reserve_pages(struct d
 
 static void client_free_pages(struct dm_kcopyd_client *kc)
 {
+	dm_enter
 	BUG_ON(kc->nr_free_pages != kc->nr_reserved_pages);
 	drop_pages(kc->pages);
 	kc->pages = NULL;
@@ -364,6 +372,7 @@ static struct kmem_cache *_job_cache;
 
 int __init dm_kcopyd_init(void)
 {
+	dm_enter
 	_job_cache = kmem_cache_create("kcopyd_job",
 				sizeof(struct kcopyd_job) * (SPLIT_COUNT + 1),
 				__alignof__(struct kcopyd_job), 0, NULL);
@@ -378,6 +387,7 @@ int __init dm_kcopyd_init(void)
 
 void dm_kcopyd_exit(void)
 {
+	dm_enter
 	kmem_cache_destroy(_job_cache);
 	_job_cache = NULL;
 }
@@ -392,6 +402,7 @@ static struct kcopyd_job *pop(struct lis
 	struct kcopyd_job *job = NULL;
 	unsigned long flags;
 
+	dm_enter
 	spin_lock_irqsave(&kc->job_lock, flags);
 
 	if (!list_empty(jobs)) {
@@ -408,6 +419,7 @@ static void push(struct list_head *jobs,
 	unsigned long flags;
 	struct dm_kcopyd_client *kc = job->kc;
 
+	dm_enter
 	spin_lock_irqsave(&kc->job_lock, flags);
 	list_add_tail(&job->list, jobs);
 	spin_unlock_irqrestore(&kc->job_lock, flags);
@@ -419,6 +431,7 @@ static void push_head(struct list_head *
 	unsigned long flags;
 	struct dm_kcopyd_client *kc = job->kc;
 
+	dm_enter
 	spin_lock_irqsave(&kc->job_lock, flags);
 	list_add(&job->list, jobs);
 	spin_unlock_irqrestore(&kc->job_lock, flags);
@@ -441,6 +454,7 @@ static int run_complete_job(struct kcopy
 	dm_kcopyd_notify_fn fn = job->fn;
 	struct dm_kcopyd_client *kc = job->kc;
 
+	dm_enter
 	if (job->pages && job->pages != &zero_page_list)
 		kcopyd_put_pages(kc, job->pages);
 	/*
@@ -461,6 +475,7 @@ static void complete_io(unsigned long er
 {
 	struct kcopyd_job *job = (struct kcopyd_job *) context;
 	struct dm_kcopyd_client *kc = job->kc;
+	dm_enter
 
 	io_job_finish(kc->throttle);
 
@@ -507,6 +522,7 @@ static int run_io_job(struct kcopyd_job
 
 	io_job_start(job->kc->throttle);
 
+	dm_enter
 	if (job->rw == READ)
 		r = dm_io(&io_req, 1, &job->source, NULL);
 	else
@@ -520,6 +536,7 @@ static int run_pages_job(struct kcopyd_j
 	int r;
 	unsigned nr_pages = dm_div_up(job->dests[0].count, PAGE_SIZE >> 9);
 
+	dm_enter
 	r = kcopyd_get_pages(job->kc, nr_pages, &job->pages);
 	if (!r) {
 		/* this job is ready for io */
@@ -544,6 +561,7 @@ static int process_jobs(struct list_head
 	struct kcopyd_job *job;
 	int r, count = 0;
 
+	dm_enter
 	while ((job = pop(jobs, kc))) {
 
 		r = fn(job);
@@ -582,6 +600,7 @@ static void do_work(struct work_struct *
 					struct dm_kcopyd_client, kcopyd_work);
 	struct blk_plug plug;
 
+	dm_enter
 	/*
 	 * The order that these are called is *very* important.
 	 * complete jobs can free some pages for pages jobs.
@@ -604,6 +623,7 @@ static void do_work(struct work_struct *
 static void dispatch_job(struct kcopyd_job *job)
 {
 	struct dm_kcopyd_client *kc = job->kc;
+	dm_enter
 	atomic_inc(&kc->nr_jobs);
 	if (unlikely(!job->source.count))
 		push(&kc->complete_jobs, job);
@@ -624,6 +644,7 @@ static void segment_complete(int read_er
 	struct kcopyd_job *job = sub_job->master_job;
 	struct dm_kcopyd_client *kc = job->kc;
 
+	dm_enter
 	mutex_lock(&job->lock);
 
 	/* update the error */
@@ -688,6 +709,7 @@ static void segment_complete(int read_er
 static void split_job(struct kcopyd_job *master_job)
 {
 	int i;
+	dm_enter
 
 	atomic_inc(&master_job->kc->nr_jobs);
 
@@ -705,6 +727,7 @@ int dm_kcopyd_copy(struct dm_kcopyd_clie
 	struct kcopyd_job *job;
 	int i;
 
+	dm_enter
 	/*
 	 * Allocate an array of jobs consisting of one master job
 	 * followed by SPLIT_COUNT sub jobs.
@@ -762,6 +785,7 @@ int dm_kcopyd_zero(struct dm_kcopyd_clie
 		   unsigned num_dests, struct dm_io_region *dests,
 		   unsigned flags, dm_kcopyd_notify_fn fn, void *context)
 {
+	dm_enter
 	return dm_kcopyd_copy(kc, NULL, num_dests, dests, flags, fn, context);
 }
 EXPORT_SYMBOL(dm_kcopyd_zero);
@@ -771,6 +795,7 @@ void *dm_kcopyd_prepare_callback(struct
 {
 	struct kcopyd_job *job;
 
+	dm_enter
 	job = mempool_alloc(kc->job_pool, GFP_NOIO);
 
 	memset(job, 0, sizeof(struct kcopyd_job));
@@ -790,6 +815,7 @@ void dm_kcopyd_do_callback(void *j, int
 	struct kcopyd_job *job = j;
 	struct dm_kcopyd_client *kc = job->kc;
 
+	dm_enter
 	job->read_err = read_err;
 	job->write_err = write_err;
 
@@ -818,6 +844,7 @@ struct dm_kcopyd_client *dm_kcopyd_clien
 	int r = -ENOMEM;
 	struct dm_kcopyd_client *kc;
 
+	dm_enter
 	kc = kmalloc(sizeof(*kc), GFP_KERNEL);
 	if (!kc)
 		return ERR_PTR(-ENOMEM);
@@ -870,6 +897,7 @@ EXPORT_SYMBOL(dm_kcopyd_client_create);
 
 void dm_kcopyd_client_destroy(struct dm_kcopyd_client *kc)
 {
+	dm_enter
 	/* Wait for completion of all jobs submitted by this client. */
 	wait_event(kc->destroyq, !atomic_read(&kc->nr_jobs));
 
Index: linux-latency/drivers/md/dm-linear.c
===================================================================
--- linux-latency.orig/drivers/md/dm-linear.c
+++ linux-latency/drivers/md/dm-linear.c
@@ -31,6 +31,7 @@ static int linear_ctr(struct dm_target *
 	unsigned long long tmp;
 	char dummy;
 
+	dm_enter
 	if (argc != 2) {
 		ti->error = "Invalid argument count";
 		return -EINVAL;
@@ -68,6 +69,7 @@ static void linear_dtr(struct dm_target
 {
 	struct linear_c *lc = (struct linear_c *) ti->private;
 
+	dm_enter
 	dm_put_device(ti, lc->dev);
 	kfree(lc);
 }
@@ -76,6 +78,7 @@ static sector_t linear_map_sector(struct
 {
 	struct linear_c *lc = ti->private;
 
+	dm_enter
 	return lc->start + dm_target_offset(ti, bi_sector);
 }
 
@@ -83,6 +86,7 @@ static void linear_map_bio(struct dm_tar
 {
 	struct linear_c *lc = ti->private;
 
+	dm_enter
 	bio->bi_bdev = lc->dev->bdev;
 	if (bio_sectors(bio))
 		bio->bi_sector = linear_map_sector(ti, bio->bi_sector);
@@ -90,6 +94,7 @@ static void linear_map_bio(struct dm_tar
 
 static int linear_map(struct dm_target *ti, struct bio *bio)
 {
+	dm_enter
 	linear_map_bio(ti, bio);
 
 	return DM_MAPIO_REMAPPED;
@@ -100,6 +105,7 @@ static void linear_status(struct dm_targ
 {
 	struct linear_c *lc = (struct linear_c *) ti->private;
 
+	dm_enter
 	switch (type) {
 	case STATUSTYPE_INFO:
 		result[0] = '\0';
@@ -119,6 +125,7 @@ static int linear_ioctl(struct dm_target
 	struct dm_dev *dev = lc->dev;
 	int r = 0;
 
+	dm_enter
 	/*
 	 * Only pass ioctls through if the device sizes match exactly.
 	 */
@@ -135,6 +142,7 @@ static int linear_merge(struct dm_target
 	struct linear_c *lc = ti->private;
 	struct request_queue *q = bdev_get_queue(lc->dev->bdev);
 
+	dm_enter
 	if (!q->merge_bvec_fn)
 		return max_size;
 
@@ -149,6 +157,7 @@ static int linear_iterate_devices(struct
 {
 	struct linear_c *lc = ti->private;
 
+	dm_enter
 	return fn(ti, lc->dev, lc->start, ti->len, data);
 }
 
@@ -167,7 +176,9 @@ static struct target_type linear_target
 
 int __init dm_linear_init(void)
 {
-	int r = dm_register_target(&linear_target);
+	int r;
+	dm_enter
+       	r = dm_register_target(&linear_target);
 
 	if (r < 0)
 		DMERR("register failed %d", r);
@@ -177,5 +188,6 @@ int __init dm_linear_init(void)
 
 void dm_linear_exit(void)
 {
+	dm_enter
 	dm_unregister_target(&linear_target);
 }
Index: linux-latency/drivers/md/dm-sysfs.c
===================================================================
--- linux-latency.orig/drivers/md/dm-sysfs.c
+++ linux-latency/drivers/md/dm-sysfs.c
@@ -25,6 +25,7 @@ static ssize_t dm_attr_show(struct kobje
 	struct mapped_device *md;
 	ssize_t ret;
 
+	dm_enter
 	dm_attr = container_of(attr, struct dm_sysfs_attr, attr);
 	if (!dm_attr->show)
 		return -EIO;
@@ -41,6 +42,7 @@ static ssize_t dm_attr_show(struct kobje
 
 static ssize_t dm_attr_name_show(struct mapped_device *md, char *buf)
 {
+	dm_enter
 	if (dm_copy_name_and_uuid(md, buf, NULL))
 		return -EIO;
 
@@ -50,6 +52,7 @@ static ssize_t dm_attr_name_show(struct
 
 static ssize_t dm_attr_uuid_show(struct mapped_device *md, char *buf)
 {
+	dm_enter
 	if (dm_copy_name_and_uuid(md, NULL, buf))
 		return -EIO;
 
@@ -59,6 +62,7 @@ static ssize_t dm_attr_uuid_show(struct
 
 static ssize_t dm_attr_suspended_show(struct mapped_device *md, char *buf)
 {
+	dm_enter
 	sprintf(buf, "%d\n", dm_suspended_md(md));
 
 	return strlen(buf);
@@ -94,6 +98,7 @@ static struct kobj_type dm_ktype = {
  */
 int dm_sysfs_init(struct mapped_device *md)
 {
+	dm_enter
 	return kobject_init_and_add(dm_kobject(md), &dm_ktype,
 				    &disk_to_dev(dm_disk(md))->kobj,
 				    "%s", "dm");
@@ -104,5 +109,6 @@ int dm_sysfs_init(struct mapped_device *
  */
 void dm_sysfs_exit(struct mapped_device *md)
 {
+	dm_enter
 	kobject_put(dm_kobject(md));
 }
Index: linux-latency/drivers/md/dm-table.c
===================================================================
--- linux-latency.orig/drivers/md/dm-table.c
+++ linux-latency/drivers/md/dm-table.c
@@ -84,6 +84,7 @@ static unsigned int int_log(unsigned int
 {
 	int result = 0;
 
+	dm_enter
 	while (n > 1) {
 		n = dm_div_up(n, base);
 		result++;
@@ -97,6 +98,7 @@ static unsigned int int_log(unsigned int
  */
 static inline unsigned int get_child(unsigned int n, unsigned int k)
 {
+	dm_enter
 	return (n * CHILDREN_PER_NODE) + k;
 }
 
@@ -106,6 +108,7 @@ static inline unsigned int get_child(uns
 static inline sector_t *get_node(struct dm_table *t,
 				 unsigned int l, unsigned int n)
 {
+	dm_enter
 	return t->index[l] + (n * KEYS_PER_NODE);
 }
 
@@ -115,6 +118,7 @@ static inline sector_t *get_node(struct
  */
 static sector_t high(struct dm_table *t, unsigned int l, unsigned int n)
 {
+	dm_enter
 	for (; l < t->depth - 1; l++)
 		n = get_child(n, CHILDREN_PER_NODE - 1);
 
@@ -132,6 +136,7 @@ static int setup_btree_index(unsigned in
 {
 	unsigned int n, k;
 	sector_t *node;
+	dm_enter
 
 	for (n = 0U; n < t->counts[l]; n++) {
 		node = get_node(t, l, n);
@@ -148,6 +153,7 @@ void *dm_vcalloc(unsigned long nmemb, un
 	unsigned long size;
 	void *addr;
 
+	dm_enter
 	/*
 	 * Check that we're not going to overflow.
 	 */
@@ -171,6 +177,7 @@ static int alloc_targets(struct dm_table
 	struct dm_target *n_targets;
 	int n = t->num_targets;
 
+	dm_enter
 	/*
 	 * Allocate both the target array and offset array at once.
 	 * Append an empty entry to catch sectors beyond the end of
@@ -202,6 +209,7 @@ int dm_table_create(struct dm_table **re
 		    unsigned num_targets, struct mapped_device *md)
 {
 	struct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);
+	dm_enter
 
 	if (!t)
 		return -ENOMEM;
@@ -230,6 +238,7 @@ static void free_devices(struct list_hea
 {
 	struct list_head *tmp, *next;
 
+	dm_enter
 	list_for_each_safe(tmp, next, devices) {
 		struct dm_dev_internal *dd =
 		    list_entry(tmp, struct dm_dev_internal, list);
@@ -243,6 +252,7 @@ void dm_table_destroy(struct dm_table *t
 {
 	unsigned int i;
 
+	dm_enter
 	if (!t)
 		return;
 
@@ -276,12 +286,14 @@ void dm_table_destroy(struct dm_table *t
 
 void dm_table_get(struct dm_table *t)
 {
+	dm_enter
 	atomic_inc(&t->holders);
 }
 EXPORT_SYMBOL(dm_table_get);
 
 void dm_table_put(struct dm_table *t)
 {
+	dm_enter
 	if (!t)
 		return;
 
@@ -295,6 +307,7 @@ EXPORT_SYMBOL(dm_table_put);
  */
 static inline int check_space(struct dm_table *t)
 {
+	dm_enter
 	if (t->num_targets >= t->num_allocated)
 		return alloc_targets(t, t->num_allocated * 2);
 
@@ -308,6 +321,7 @@ static struct dm_dev_internal *find_devi
 {
 	struct dm_dev_internal *dd;
 
+	dm_enter
 	list_for_each_entry (dd, l, list)
 		if (dd->dm_dev.bdev->bd_dev == dev)
 			return dd;
@@ -326,6 +340,7 @@ static int open_dev(struct dm_dev_intern
 
 	int r;
 
+	dm_enter
 	BUG_ON(d->dm_dev.bdev);
 
 	bdev = blkdev_get_by_dev(dev, d->dm_dev.mode | FMODE_EXCL, _claim_ptr);
@@ -347,6 +362,7 @@ static int open_dev(struct dm_dev_intern
  */
 static void close_dev(struct dm_dev_internal *d, struct mapped_device *md)
 {
+	dm_enter
 	if (!d->dm_dev.bdev)
 		return;
 
@@ -370,6 +386,7 @@ static int device_area_is_invalid(struct
 		limits->logical_block_size >> SECTOR_SHIFT;
 	char b[BDEVNAME_SIZE];
 
+	dm_enter
 	/*
 	 * Some devices exist without request functions,
 	 * such as loop devices not yet bound to backing files.
@@ -435,6 +452,7 @@ static int upgrade_mode(struct dm_dev_in
 	int r;
 	struct dm_dev_internal dd_new, dd_old;
 
+	dm_enter
 	dd_new = dd_old = *dd;
 
 	dd_new.dm_dev.mode |= new_mode;
@@ -464,6 +482,7 @@ int dm_get_device(struct dm_target *ti,
 	struct dm_table *t = ti->table;
 	char dummy;
 
+	dm_enter
 	BUG_ON(!t);
 
 	if (sscanf(path, "%u:%u%c", &major, &minor, &dummy) == 2) {
@@ -519,6 +538,7 @@ int dm_set_device_limits(struct dm_targe
 	struct block_device *bdev = dev->bdev;
 	struct request_queue *q = bdev_get_queue(bdev);
 	char b[BDEVNAME_SIZE];
+	dm_enter
 
 	if (unlikely(!q)) {
 		DMWARN("%s: Cannot set limits for nonexistent device %s",
@@ -556,6 +576,7 @@ void dm_put_device(struct dm_target *ti,
 	struct dm_dev_internal *dd = container_of(d, struct dm_dev_internal,
 						  dm_dev);
 
+	dm_enter
 	if (atomic_dec_and_test(&dd->count)) {
 		close_dev(dd, ti->table->md);
 		list_del(&dd->list);
@@ -570,6 +591,7 @@ EXPORT_SYMBOL(dm_put_device);
 static int adjoin(struct dm_table *table, struct dm_target *ti)
 {
 	struct dm_target *prev;
+	dm_enter
 
 	if (!table->num_targets)
 		return !ti->begin;
@@ -586,6 +608,7 @@ static char **realloc_argv(unsigned *arr
 	char **argv;
 	unsigned new_size;
 
+	dm_enter
 	new_size = *array_size ? *array_size * 2 : 64;
 	argv = kmalloc(new_size * sizeof(*argv), GFP_KERNEL);
 	if (argv) {
@@ -605,6 +628,7 @@ int dm_split_args(int *argc, char ***arg
 	char *start, *end = input, *out, **argv = NULL;
 	unsigned array_size = 0;
 
+	dm_enter
 	*argc = 0;
 
 	if (!input) {
@@ -692,6 +716,7 @@ static int validate_hardware_logical_blo
 	struct queue_limits ti_limits;
 	unsigned i = 0;
 
+	dm_enter
 	/*
 	 * Check each entry in the table in turn.
 	 */
@@ -741,6 +766,7 @@ int dm_table_add_target(struct dm_table
 	char **argv;
 	struct dm_target *tgt;
 
+	dm_enter
 	if (t->singleton) {
 		DMERR("%s: target type %s must appear alone in table",
 		      dm_device_name(t->md), t->targets->type->name);
@@ -843,6 +869,7 @@ static int validate_next_arg(struct dm_a
 	const char *arg_str = dm_shift_arg(arg_set);
 	char dummy;
 
+	dm_enter
 	if (!arg_str ||
 	    (sscanf(arg_str, "%u%c", value, &dummy) != 1) ||
 	    (*value < arg->min) ||
@@ -858,6 +885,7 @@ static int validate_next_arg(struct dm_a
 int dm_read_arg(struct dm_arg *arg, struct dm_arg_set *arg_set,
 		unsigned *value, char **error)
 {
+	dm_enter
 	return validate_next_arg(arg, arg_set, value, error, 0);
 }
 EXPORT_SYMBOL(dm_read_arg);
@@ -865,6 +893,7 @@ EXPORT_SYMBOL(dm_read_arg);
 int dm_read_arg_group(struct dm_arg *arg, struct dm_arg_set *arg_set,
 		      unsigned *value, char **error)
 {
+	dm_enter
 	return validate_next_arg(arg, arg_set, value, error, 1);
 }
 EXPORT_SYMBOL(dm_read_arg_group);
@@ -873,6 +902,7 @@ const char *dm_shift_arg(struct dm_arg_s
 {
 	char *r;
 
+	dm_enter
 	if (as->argc) {
 		as->argc--;
 		r = *as->argv;
@@ -886,6 +916,7 @@ EXPORT_SYMBOL(dm_shift_arg);
 
 void dm_consume_args(struct dm_arg_set *as, unsigned num_args)
 {
+	dm_enter
 	BUG_ON(as->argc < num_args);
 	as->argc -= num_args;
 	as->argv += num_args;
@@ -900,6 +931,7 @@ static int dm_table_set_type(struct dm_t
 	struct dm_dev_internal *dd;
 	struct list_head *devices;
 
+	dm_enter
 	for (i = 0; i < t->num_targets; i++) {
 		tgt = t->targets + i;
 		if (dm_target_request_based(tgt))
@@ -950,16 +982,19 @@ static int dm_table_set_type(struct dm_t
 
 unsigned dm_table_get_type(struct dm_table *t)
 {
+	dm_enter
 	return t->type;
 }
 
 struct target_type *dm_table_get_immutable_target_type(struct dm_table *t)
 {
+	dm_enter
 	return t->immutable_target_type;
 }
 
 bool dm_table_request_based(struct dm_table *t)
 {
+	dm_enter
 	return dm_table_get_type(t) == DM_TYPE_REQUEST_BASED;
 }
 
@@ -969,6 +1004,7 @@ int dm_table_alloc_md_mempools(struct dm
 	unsigned per_bio_data_size = 0;
 	struct dm_target *tgt;
 	unsigned i;
+	dm_enter
 
 	if (unlikely(type == DM_TYPE_NONE)) {
 		DMWARN("no table type is set, can't allocate mempools");
@@ -996,6 +1032,7 @@ void dm_table_free_md_mempools(struct dm
 
 struct dm_md_mempools *dm_table_get_md_mempools(struct dm_table *t)
 {
+	dm_enter
 	return t->mempools;
 }
 
@@ -1005,6 +1042,7 @@ static int setup_indexes(struct dm_table
 	unsigned int total = 0;
 	sector_t *indexes;
 
+	dm_enter
 	/* allocate the space for *all* the indexes */
 	for (i = t->depth - 2; i >= 0; i--) {
 		t->counts[i] = dm_div_up(t->counts[i + 1], CHILDREN_PER_NODE);
@@ -1033,6 +1071,7 @@ static int dm_table_build_index(struct d
 	int r = 0;
 	unsigned int leaf_nodes;
 
+	dm_enter
 	/* how many indexes will the btree have ? */
 	leaf_nodes = dm_div_up(t->num_targets, KEYS_PER_NODE);
 	t->depth = 1 + int_log(leaf_nodes, CHILDREN_PER_NODE);
@@ -1061,6 +1100,7 @@ static struct gendisk * dm_table_get_int
 	struct dm_dev_internal *dd = NULL;
 	struct gendisk *prev_disk = NULL, *template_disk = NULL;
 
+	dm_enter
 	list_for_each_entry(dd, devices, list) {
 		template_disk = dd->dm_dev.bdev->bd_disk;
 		if (!blk_get_integrity(template_disk))
@@ -1098,6 +1138,7 @@ static int dm_table_prealloc_integrity(s
 {
 	struct gendisk *template_disk = NULL;
 
+	dm_enter
 	template_disk = dm_table_get_integrity_disk(t, false);
 	if (!template_disk)
 		return 0;
@@ -1133,6 +1174,7 @@ int dm_table_complete(struct dm_table *t
 {
 	int r;
 
+	dm_enter
 	r = dm_table_set_type(t);
 	if (r) {
 		DMERR("unable to set table type");
@@ -1162,6 +1204,7 @@ static DEFINE_MUTEX(_event_lock);
 void dm_table_event_callback(struct dm_table *t,
 			     void (*fn)(void *), void *context)
 {
+	dm_enter
 	mutex_lock(&_event_lock);
 	t->event_fn = fn;
 	t->event_context = context;
@@ -1170,6 +1213,7 @@ void dm_table_event_callback(struct dm_t
 
 void dm_table_event(struct dm_table *t)
 {
+	dm_enter
 	/*
 	 * You can no longer call dm_table_event() from interrupt
 	 * context, use a bottom half instead.
@@ -1185,12 +1229,14 @@ EXPORT_SYMBOL(dm_table_event);
 
 sector_t dm_table_get_size(struct dm_table *t)
 {
+	dm_enter
 	return t->num_targets ? (t->highs[t->num_targets - 1] + 1) : 0;
 }
 EXPORT_SYMBOL(dm_table_get_size);
 
 struct dm_target *dm_table_get_target(struct dm_table *t, unsigned int index)
 {
+	dm_enter
 	if (index >= t->num_targets)
 		return NULL;
 
@@ -1207,6 +1253,7 @@ struct dm_target *dm_table_find_target(s
 {
 	unsigned int l, n = 0, k = 0;
 	sector_t *node;
+	dm_enter
 
 	for (l = 0; l < t->depth; l++) {
 		n = get_child(n, k);
@@ -1224,6 +1271,7 @@ static int count_device(struct dm_target
 			sector_t start, sector_t len, void *data)
 {
 	unsigned *num_devices = data;
+	dm_enter
 
 	(*num_devices)++;
 
@@ -1241,6 +1289,7 @@ bool dm_table_has_no_data_devices(struct
 	struct dm_target *uninitialized_var(ti);
 	unsigned i = 0, num_devices = 0;
 
+	dm_enter
 	while (i < dm_table_get_num_targets(table)) {
 		ti = dm_table_get_target(table, i++);
 
@@ -1264,6 +1313,7 @@ int dm_calculate_queue_limits(struct dm_
 	struct dm_target *uninitialized_var(ti);
 	struct queue_limits ti_limits;
 	unsigned i = 0;
+	dm_enter
 
 	blk_set_stacking_limits(limits);
 
@@ -1321,6 +1371,7 @@ static void dm_table_set_integrity(struc
 {
 	struct gendisk *template_disk = NULL;
 
+	dm_enter
 	if (!blk_get_integrity(dm_disk(t->md)))
 		return;
 
@@ -1342,6 +1393,7 @@ static int device_flush_capable(struct d
 	unsigned flush = (*(unsigned *)data);
 	struct request_queue *q = bdev_get_queue(dev->bdev);
 
+	dm_enter
 	return q && (q->flush_flags & flush);
 }
 
@@ -1350,6 +1402,7 @@ static bool dm_table_supports_flush(stru
 	struct dm_target *ti;
 	unsigned i = 0;
 
+	dm_enter
 	/*
 	 * Require at least one underlying device to support flushes.
 	 * t->devices includes internal dm devices such as mirror logs
@@ -1378,6 +1431,7 @@ static bool dm_table_discard_zeroes_data
 	struct dm_target *ti;
 	unsigned i = 0;
 
+	dm_enter
 	/* Ensure that all targets supports discard_zeroes_data. */
 	while (i < dm_table_get_num_targets(t)) {
 		ti = dm_table_get_target(t, i++);
@@ -1394,6 +1448,7 @@ static int device_is_nonrot(struct dm_ta
 {
 	struct request_queue *q = bdev_get_queue(dev->bdev);
 
+	dm_enter
 	return q && blk_queue_nonrot(q);
 }
 
@@ -1402,6 +1457,7 @@ static int device_is_not_random(struct d
 {
 	struct request_queue *q = bdev_get_queue(dev->bdev);
 
+	dm_enter
 	return q && !blk_queue_add_random(q);
 }
 
@@ -1411,6 +1467,7 @@ static bool dm_table_all_devices_attribu
 	struct dm_target *ti;
 	unsigned i = 0;
 
+	dm_enter
 	while (i < dm_table_get_num_targets(t)) {
 		ti = dm_table_get_target(t, i++);
 
@@ -1427,6 +1484,7 @@ static int device_not_write_same_capable
 {
 	struct request_queue *q = bdev_get_queue(dev->bdev);
 
+	dm_enter
 	return q && !q->limits.max_write_same_sectors;
 }
 
@@ -1435,6 +1493,7 @@ static bool dm_table_supports_write_same
 	struct dm_target *ti;
 	unsigned i = 0;
 
+	dm_enter
 	while (i < dm_table_get_num_targets(t)) {
 		ti = dm_table_get_target(t, i++);
 
@@ -1454,6 +1513,7 @@ void dm_table_set_restrictions(struct dm
 {
 	unsigned flush = 0;
 
+	dm_enter
 	/*
 	 * Copy table's limits to the DM device's request_queue
 	 */
@@ -1510,16 +1570,19 @@ void dm_table_set_restrictions(struct dm
 
 unsigned int dm_table_get_num_targets(struct dm_table *t)
 {
+	dm_enter
 	return t->num_targets;
 }
 
 struct list_head *dm_table_get_devices(struct dm_table *t)
 {
+	dm_enter
 	return &t->devices;
 }
 
 fmode_t dm_table_get_mode(struct dm_table *t)
 {
+	dm_enter
 	return t->mode;
 }
 EXPORT_SYMBOL(dm_table_get_mode);
@@ -1529,6 +1592,7 @@ static void suspend_targets(struct dm_ta
 	int i = t->num_targets;
 	struct dm_target *ti = t->targets;
 
+	dm_enter
 	while (i--) {
 		if (postsuspend) {
 			if (ti->type->postsuspend)
@@ -1542,6 +1606,7 @@ static void suspend_targets(struct dm_ta
 
 void dm_table_presuspend_targets(struct dm_table *t)
 {
+	dm_enter
 	if (!t)
 		return;
 
@@ -1550,6 +1615,7 @@ void dm_table_presuspend_targets(struct
 
 void dm_table_postsuspend_targets(struct dm_table *t)
 {
+	dm_enter
 	if (!t)
 		return;
 
@@ -1560,6 +1626,7 @@ int dm_table_resume_targets(struct dm_ta
 {
 	int i, r = 0;
 
+	dm_enter
 	for (i = 0; i < t->num_targets; i++) {
 		struct dm_target *ti = t->targets + i;
 
@@ -1583,6 +1650,7 @@ int dm_table_resume_targets(struct dm_ta
 
 void dm_table_add_target_callbacks(struct dm_table *t, struct dm_target_callbacks *cb)
 {
+	dm_enter
 	list_add(&cb->list, &t->target_callbacks);
 }
 EXPORT_SYMBOL_GPL(dm_table_add_target_callbacks);
@@ -1594,6 +1662,7 @@ int dm_table_any_congested(struct dm_tab
 	struct dm_target_callbacks *cb;
 	int r = 0;
 
+	dm_enter
 	list_for_each_entry(dd, devices, list) {
 		struct request_queue *q = bdev_get_queue(dd->dm_dev.bdev);
 		char b[BDEVNAME_SIZE];
@@ -1618,6 +1687,7 @@ int dm_table_any_busy_target(struct dm_t
 	unsigned i;
 	struct dm_target *ti;
 
+	dm_enter
 	for (i = 0; i < t->num_targets; i++) {
 		ti = t->targets + i;
 		if (ti->type->busy && ti->type->busy(ti))
@@ -1629,6 +1699,7 @@ int dm_table_any_busy_target(struct dm_t
 
 struct mapped_device *dm_table_get_md(struct dm_table *t)
 {
+	dm_enter
 	return t->md;
 }
 EXPORT_SYMBOL(dm_table_get_md);
@@ -1638,6 +1709,7 @@ static int device_discard_capable(struct
 {
 	struct request_queue *q = bdev_get_queue(dev->bdev);
 
+	dm_enter
 	return q && blk_queue_discard(q);
 }
 
@@ -1646,6 +1718,7 @@ bool dm_table_supports_discards(struct d
 	struct dm_target *ti;
 	unsigned i = 0;
 
+	dm_enter
 	/*
 	 * Unless any target used by the table set discards_supported,
 	 * require at least one underlying device to support discards.
Index: linux-latency/drivers/md/dm-target.c
===================================================================
--- linux-latency.orig/drivers/md/dm-target.c
+++ linux-latency/drivers/md/dm-target.c
@@ -22,6 +22,7 @@ static inline struct target_type *__find
 {
 	struct target_type *tt;
 
+	dm_enter
 	list_for_each_entry(tt, &_targets, list)
 		if (!strcmp(name, tt->name))
 			return tt;
@@ -33,6 +34,7 @@ static struct target_type *get_target_ty
 {
 	struct target_type *tt;
 
+	dm_enter
 	down_read(&_lock);
 
 	tt = __find_target_type(name);
@@ -52,6 +54,7 @@ struct target_type *dm_get_target_type(c
 {
 	struct target_type *tt = get_target_type(name);
 
+	dm_enter
 	if (!tt) {
 		load_module(name);
 		tt = get_target_type(name);
@@ -62,6 +65,7 @@ struct target_type *dm_get_target_type(c
 
 void dm_put_target_type(struct target_type *tt)
 {
+	dm_enter
 	down_read(&_lock);
 	module_put(tt->module);
 	up_read(&_lock);
@@ -72,6 +76,7 @@ int dm_target_iterate(void (*iter_func)(
 {
 	struct target_type *tt;
 
+	dm_enter
 	down_read(&_lock);
 	list_for_each_entry(tt, &_targets, list)
 		iter_func(tt, param);
@@ -84,6 +89,7 @@ int dm_register_target(struct target_typ
 {
 	int rv = 0;
 
+	dm_enter
 	down_write(&_lock);
 	if (__find_target_type(tt->name))
 		rv = -EEXIST;
@@ -96,6 +102,7 @@ int dm_register_target(struct target_typ
 
 void dm_unregister_target(struct target_type *tt)
 {
+	dm_enter
 	down_write(&_lock);
 	if (!__find_target_type(tt->name)) {
 		DMCRIT("Unregistering unrecognised target: %s", tt->name);
@@ -113,6 +120,7 @@ void dm_unregister_target(struct target_
  */
 static int io_err_ctr(struct dm_target *tt, unsigned int argc, char **args)
 {
+	dm_enter
 	/*
 	 * Return error for discards instead of -EOPNOTSUPP
 	 */
@@ -123,11 +131,13 @@ static int io_err_ctr(struct dm_target *
 
 static void io_err_dtr(struct dm_target *tt)
 {
+	dm_enter
 	/* empty */
 }
 
 static int io_err_map(struct dm_target *tt, struct bio *bio)
 {
+	dm_enter
 	return -EIO;
 }
 
@@ -141,11 +151,13 @@ static struct target_type error_target =
 
 int __init dm_target_init(void)
 {
+	dm_enter
 	return dm_register_target(&error_target);
 }
 
 void dm_target_exit(void)
 {
+	dm_enter
 	dm_unregister_target(&error_target);
 }
 
Index: linux-latency/drivers/md/dm-uevent.c
===================================================================
--- linux-latency.orig/drivers/md/dm-uevent.c
+++ linux-latency/drivers/md/dm-uevent.c
@@ -58,6 +58,7 @@ static struct dm_uevent *dm_uevent_alloc
 {
 	struct dm_uevent *event;
 
+	dm_enter
 	event = kmem_cache_zalloc(_dm_event_cache, GFP_ATOMIC);
 	if (!event)
 		return NULL;
@@ -77,6 +78,7 @@ static struct dm_uevent *dm_build_path_u
 {
 	struct dm_uevent *event;
 
+	dm_enter
 	event = dm_uevent_alloc(md);
 	if (!event) {
 		DMERR("%s: dm_uevent_alloc() failed", __func__);
@@ -136,6 +138,7 @@ void dm_send_uevents(struct list_head *e
 	int r;
 	struct dm_uevent *event, *next;
 
+	dm_enter
 	list_for_each_entry_safe(event, next, events, elist) {
 		list_del_init(&event->elist);
 
@@ -186,6 +189,7 @@ void dm_path_uevent(enum dm_uevent_type
 	struct mapped_device *md = dm_table_get_md(ti->table);
 	struct dm_uevent *event;
 
+	dm_enter
 	if (event_type >= ARRAY_SIZE(_dm_uevent_type_names)) {
 		DMERR("%s: Invalid event_type %d", __func__, event_type);
 		return;
@@ -204,6 +208,7 @@ EXPORT_SYMBOL_GPL(dm_path_uevent);
 
 int dm_uevent_init(void)
 {
+	dm_enter
 	_dm_event_cache = KMEM_CACHE(dm_uevent, 0);
 	if (!_dm_event_cache)
 		return -ENOMEM;
@@ -215,5 +220,6 @@ int dm_uevent_init(void)
 
 void dm_uevent_exit(void)
 {
+	dm_enter
 	kmem_cache_destroy(_dm_event_cache);
 }
Index: linux-latency/drivers/md/dm.c
===================================================================
--- linux-latency.orig/drivers/md/dm.c
+++ linux-latency/drivers/md/dm.c
@@ -90,6 +90,7 @@ struct dm_rq_clone_bio_info {
 
 union map_info *dm_get_mapinfo(struct bio *bio)
 {
+	dm_enter
 	if (bio && bio->bi_private)
 		return &((struct dm_target_io *)bio->bi_private)->info;
 	return NULL;
@@ -97,6 +98,7 @@ union map_info *dm_get_mapinfo(struct bi
 
 union map_info *dm_get_rq_mapinfo(struct request *rq)
 {
+	dm_enter
 	if (rq && rq->end_io_data)
 		return &((struct dm_rq_target_io *)rq->end_io_data)->info;
 	return NULL;
@@ -207,6 +209,7 @@ static int __init local_init(void)
 {
 	int r = -ENOMEM;
 
+	dm_enter
 	/* allocate a slab for the dm_ios */
 	_io_cache = KMEM_CACHE(dm_io, 0);
 	if (!_io_cache)
@@ -242,6 +245,7 @@ out_free_io_cache:
 
 static void local_exit(void)
 {
+	dm_enter
 	kmem_cache_destroy(_rq_tio_cache);
 	kmem_cache_destroy(_io_cache);
 	unregister_blkdev(_major, _name);
@@ -278,6 +282,7 @@ static int __init dm_init(void)
 
 	int r, i;
 
+	dm_enter
 	for (i = 0; i < count; i++) {
 		r = _inits[i]();
 		if (r)
@@ -297,6 +302,7 @@ static void __exit dm_exit(void)
 {
 	int i = ARRAY_SIZE(_exits);
 
+	dm_enter
 	while (i--)
 		_exits[i]();
 
@@ -311,6 +317,7 @@ static void __exit dm_exit(void)
  */
 int dm_deleting_md(struct mapped_device *md)
 {
+	dm_enter
 	return test_bit(DMF_DELETING, &md->flags);
 }
 
@@ -318,6 +325,7 @@ static int dm_blk_open(struct block_devi
 {
 	struct mapped_device *md;
 
+	dm_enter
 	spin_lock(&_minor_lock);
 
 	md = bdev->bd_disk->private_data;
@@ -343,6 +351,7 @@ static void dm_blk_close(struct gendisk
 {
 	struct mapped_device *md = disk->private_data;
 
+	dm_enter
 	spin_lock(&_minor_lock);
 
 	atomic_dec(&md->open_count);
@@ -353,6 +362,7 @@ static void dm_blk_close(struct gendisk
 
 int dm_open_count(struct mapped_device *md)
 {
+	dm_enter
 	return atomic_read(&md->open_count);
 }
 
@@ -363,6 +373,7 @@ int dm_lock_for_deletion(struct mapped_d
 {
 	int r = 0;
 
+	dm_enter
 	spin_lock(&_minor_lock);
 
 	if (dm_open_count(md))
@@ -379,6 +390,7 @@ static int dm_blk_getgeo(struct block_de
 {
 	struct mapped_device *md = bdev->bd_disk->private_data;
 
+	dm_enter
 	return dm_get_geometry(md, geo);
 }
 
@@ -390,6 +402,7 @@ static int dm_blk_ioctl(struct block_dev
 	struct dm_target *tgt;
 	int r = -ENOTTY;
 
+	dm_enter
 	if (!map || !dm_table_get_size(map))
 		goto out;
 
@@ -415,32 +428,38 @@ out:
 
 static struct dm_io *alloc_io(struct mapped_device *md)
 {
+	dm_enter
 	return mempool_alloc(md->io_pool, GFP_NOIO);
 }
 
 static void free_io(struct mapped_device *md, struct dm_io *io)
 {
+	dm_enter
 	mempool_free(io, md->io_pool);
 }
 
 static void free_tio(struct mapped_device *md, struct dm_target_io *tio)
 {
+	dm_enter
 	bio_put(&tio->clone);
 }
 
 static struct dm_rq_target_io *alloc_rq_tio(struct mapped_device *md,
 					    gfp_t gfp_mask)
 {
+	dm_enter
 	return mempool_alloc(md->io_pool, gfp_mask);
 }
 
 static void free_rq_tio(struct dm_rq_target_io *tio)
 {
+	dm_enter
 	mempool_free(tio, tio->md->io_pool);
 }
 
 static int md_in_flight(struct mapped_device *md)
 {
+	dm_enter
 	return atomic_read(&md->pending[READ]) +
 	       atomic_read(&md->pending[WRITE]);
 }
@@ -451,6 +470,7 @@ static void start_io_acct(struct dm_io *
 	int cpu;
 	int rw = bio_data_dir(io->bio);
 
+	dm_enter
 	io->start_time = jiffies;
 
 	cpu = part_stat_lock();
@@ -468,6 +488,7 @@ static void end_io_acct(struct dm_io *io
 	int pending, cpu;
 	int rw = bio_data_dir(bio);
 
+	dm_enter
 	cpu = part_stat_lock();
 	part_round_stats(cpu, &dm_disk(md)->part0);
 	part_stat_add(cpu, &dm_disk(md)->part0, ticks[rw], duration);
@@ -493,6 +514,7 @@ static void queue_io(struct mapped_devic
 {
 	unsigned long flags;
 
+	dm_enter
 	spin_lock_irqsave(&md->deferred_lock, flags);
 	bio_list_add(&md->deferred, bio);
 	spin_unlock_irqrestore(&md->deferred_lock, flags);
@@ -509,6 +531,7 @@ struct dm_table *dm_get_live_table(struc
 	struct dm_table *t;
 	unsigned long flags;
 
+	dm_enter
 	read_lock_irqsave(&md->map_lock, flags);
 	t = md->map;
 	if (t)
@@ -523,6 +546,7 @@ struct dm_table *dm_get_live_table(struc
  */
 int dm_get_geometry(struct mapped_device *md, struct hd_geometry *geo)
 {
+	dm_enter
 	*geo = md->geometry;
 
 	return 0;
@@ -535,6 +559,7 @@ int dm_set_geometry(struct mapped_device
 {
 	sector_t sz = (sector_t)geo->cylinders * geo->heads * geo->sectors;
 
+	dm_enter
 	if (geo->start > sz) {
 		DMWARN("Start sector is beyond the geometry limits.");
 		return -EINVAL;
@@ -556,6 +581,7 @@ int dm_set_geometry(struct mapped_device
 
 static int __noflush_suspending(struct mapped_device *md)
 {
+	dm_enter
 	return test_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);
 }
 
@@ -570,6 +596,7 @@ static void dec_pending(struct dm_io *io
 	struct bio *bio;
 	struct mapped_device *md = io->md;
 
+	dm_enter
 	/* Push-back supersedes any I/O errors */
 	if (unlikely(error)) {
 		spin_lock_irqsave(&io->endio_lock, flags);
@@ -623,6 +650,7 @@ static void clone_endio(struct bio *bio,
 	struct mapped_device *md = tio->io->md;
 	dm_endio_fn endio = tio->ti->type->end_io;
 
+	dm_enter
 	if (!bio_flagged(bio, BIO_UPTODATE) && !error)
 		error = -EIO;
 
@@ -657,6 +685,7 @@ static void end_clone_bio(struct bio *cl
 	struct bio *bio = info->orig;
 	unsigned int nr_bytes = info->orig->bi_size;
 
+	dm_enter
 	bio_put(clone);
 
 	if (tio->error)
@@ -704,6 +733,7 @@ static void end_clone_bio(struct bio *cl
  */
 static void rq_completed(struct mapped_device *md, int rw, int run_queue)
 {
+	dm_enter
 	atomic_dec(&md->pending[rw]);
 
 	/* nudge anyone waiting on suspend queue */
@@ -729,6 +759,7 @@ static void free_rq_clone(struct request
 {
 	struct dm_rq_target_io *tio = clone->end_io_data;
 
+	dm_enter
 	blk_rq_unprep_clone(clone);
 	free_rq_tio(tio);
 }
@@ -744,6 +775,7 @@ static void dm_end_request(struct reques
 	struct mapped_device *md = tio->md;
 	struct request *rq = tio->orig;
 
+	dm_enter
 	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
 		rq->errors = clone->errors;
 		rq->resid_len = clone->resid_len;
@@ -766,6 +798,7 @@ static void dm_unprep_request(struct req
 {
 	struct request *clone = rq->special;
 
+	dm_enter
 	rq->special = NULL;
 	rq->cmd_flags &= ~REQ_DONTPREP;
 
@@ -784,6 +817,7 @@ void dm_requeue_unmapped_request(struct
 	struct request_queue *q = rq->q;
 	unsigned long flags;
 
+	dm_enter
 	dm_unprep_request(rq);
 
 	spin_lock_irqsave(q->queue_lock, flags);
@@ -796,6 +830,7 @@ EXPORT_SYMBOL_GPL(dm_requeue_unmapped_re
 
 static void __stop_queue(struct request_queue *q)
 {
+	dm_enter
 	blk_stop_queue(q);
 }
 
@@ -803,6 +838,7 @@ static void stop_queue(struct request_qu
 {
 	unsigned long flags;
 
+	dm_enter
 	spin_lock_irqsave(q->queue_lock, flags);
 	__stop_queue(q);
 	spin_unlock_irqrestore(q->queue_lock, flags);
@@ -810,6 +846,7 @@ static void stop_queue(struct request_qu
 
 static void __start_queue(struct request_queue *q)
 {
+	dm_enter
 	if (blk_queue_stopped(q))
 		blk_start_queue(q);
 }
@@ -818,6 +855,7 @@ static void start_queue(struct request_q
 {
 	unsigned long flags;
 
+	dm_enter
 	spin_lock_irqsave(q->queue_lock, flags);
 	__start_queue(q);
 	spin_unlock_irqrestore(q->queue_lock, flags);
@@ -829,6 +867,7 @@ static void dm_done(struct request *clon
 	struct dm_rq_target_io *tio = clone->end_io_data;
 	dm_request_endio_fn rq_end_io = NULL;
 
+	dm_enter
 	if (tio->ti) {
 		rq_end_io = tio->ti->type->rq_end_io;
 
@@ -860,6 +899,7 @@ static void dm_softirq_done(struct reque
 	struct request *clone = rq->completion_data;
 	struct dm_rq_target_io *tio = clone->end_io_data;
 
+	dm_enter
 	if (rq->cmd_flags & REQ_FAILED)
 		mapped = false;
 
@@ -875,6 +915,7 @@ static void dm_complete_request(struct r
 	struct dm_rq_target_io *tio = clone->end_io_data;
 	struct request *rq = tio->orig;
 
+	dm_enter
 	tio->error = error;
 	rq->completion_data = clone;
 	blk_complete_request(rq);
@@ -890,6 +931,7 @@ void dm_kill_unmapped_request(struct req
 {
 	struct dm_rq_target_io *tio = clone->end_io_data;
 	struct request *rq = tio->orig;
+	dm_enter
 
 	rq->cmd_flags |= REQ_FAILED;
 	dm_complete_request(clone, error);
@@ -901,6 +943,7 @@ EXPORT_SYMBOL_GPL(dm_kill_unmapped_reque
  */
 static void end_clone_request(struct request *clone, int error)
 {
+	dm_enter
 	/*
 	 * For just cleaning up the information of the queue in which
 	 * the clone was dispatched.
@@ -927,6 +970,7 @@ static void end_clone_request(struct req
 static sector_t max_io_len_target_boundary(sector_t sector, struct dm_target *ti)
 {
 	sector_t target_offset = dm_target_offset(ti, sector);
+	dm_enter
 
 	return ti->len - target_offset;
 }
@@ -936,6 +980,7 @@ static sector_t max_io_len(sector_t sect
 	sector_t len = max_io_len_target_boundary(sector, ti);
 	sector_t offset, max_len;
 
+	dm_enter
 	/*
 	 * Does the target need to split even further?
 	 */
@@ -956,6 +1001,7 @@ static sector_t max_io_len(sector_t sect
 
 int dm_set_target_max_io_len(struct dm_target *ti, sector_t len)
 {
+	dm_enter
 	if (len > UINT_MAX) {
 		DMERR("Specified maximum size of target IO (%llu) exceeds limit (%u)",
 		      (unsigned long long)len, UINT_MAX);
@@ -977,6 +1023,7 @@ static void __map_bio(struct dm_target_i
 	struct bio *clone = &tio->clone;
 	struct dm_target *ti = tio->ti;
 
+	dm_enter
 	clone->bi_end_io = clone_endio;
 	clone->bi_private = tio;
 
@@ -1024,6 +1071,7 @@ static void bio_setup_sector(struct bio
 
 static void bio_setup_bv(struct bio *bio, unsigned short idx, unsigned short bv_count)
 {
+	dm_enter
 	bio->bi_idx = idx;
 	bio->bi_vcnt = idx + bv_count;
 	bio->bi_flags &= ~(1 << BIO_SEG_VALID);
@@ -1033,6 +1081,7 @@ static void clone_bio_integrity(struct b
 				unsigned short idx, unsigned len, unsigned offset,
 				unsigned trim)
 {
+	dm_enter
 	if (!bio_integrity(bio))
 		return;
 
@@ -1052,6 +1101,7 @@ static void clone_split_bio(struct dm_ta
 	struct bio *clone = &tio->clone;
 	struct bio_vec *bv = bio->bi_io_vec + idx;
 
+	dm_enter
 	*clone->bi_io_vec = *bv;
 
 	bio_setup_sector(clone, sector, len);
@@ -1076,6 +1126,7 @@ static void clone_bio(struct dm_target_i
 	struct bio *clone = &tio->clone;
 	unsigned trim = 0;
 
+	dm_enter
 	__bio_clone(clone, bio);
 	bio_setup_sector(clone, sector, len);
 	bio_setup_bv(clone, idx, bv_count);
@@ -1092,6 +1143,7 @@ static struct dm_target_io *alloc_tio(st
 	struct dm_target_io *tio;
 	struct bio *clone;
 
+	dm_enter
 	clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, ci->md->bs);
 	tio = container_of(clone, struct dm_target_io, clone);
 
@@ -1110,6 +1162,7 @@ static void __clone_and_map_simple_bio(s
 	struct dm_target_io *tio = alloc_tio(ci, ti, ci->bio->bi_max_vecs, target_bio_nr);
 	struct bio *clone = &tio->clone;
 
+	dm_enter
 	/*
 	 * Discard requests require the bio's inline iovecs be initialized.
 	 * ci->bio->bi_max_vecs is BIO_INLINE_VECS anyway, for both flush
@@ -1127,6 +1180,7 @@ static void __send_duplicate_bios(struct
 {
 	unsigned target_bio_nr;
 
+	dm_enter
 	for (target_bio_nr = 0; target_bio_nr < num_bios; target_bio_nr++)
 		__clone_and_map_simple_bio(ci, ti, target_bio_nr, len);
 }
@@ -1136,6 +1190,7 @@ static int __send_empty_flush(struct clo
 	unsigned target_nr = 0;
 	struct dm_target *ti;
 
+	dm_enter
 	BUG_ON(bio_has_data(ci->bio));
 	while ((ti = dm_table_get_target(ci->map, target_nr++)))
 		__send_duplicate_bios(ci, ti, ti->num_flush_bios, 0);
@@ -1154,6 +1209,7 @@ static void __clone_and_map_data_bio(str
 	unsigned target_bio_nr;
 	unsigned num_target_bios = 1;
 
+	dm_enter
 	/*
 	 * Does the target want to receive duplicate copies of the bio?
 	 */
@@ -1174,11 +1230,13 @@ typedef unsigned (*get_num_bios_fn)(stru
 
 static unsigned get_num_discard_bios(struct dm_target *ti)
 {
+	dm_enter
 	return ti->num_discard_bios;
 }
 
 static unsigned get_num_write_same_bios(struct dm_target *ti)
 {
+	dm_enter
 	return ti->num_write_same_bios;
 }
 
@@ -1186,6 +1244,7 @@ typedef bool (*is_split_required_fn)(str
 
 static bool is_split_required_for_discard(struct dm_target *ti)
 {
+	dm_enter
 	return ti->split_discard_bios;
 }
 
@@ -1197,6 +1256,7 @@ static int __send_changing_extent_only(s
 	sector_t len;
 	unsigned num_bios;
 
+	dm_enter
 	do {
 		ti = dm_table_find_target(ci->map, ci->sector);
 		if (!dm_target_is_valid(ti))
@@ -1227,12 +1287,14 @@ static int __send_changing_extent_only(s
 
 static int __send_discard(struct clone_info *ci)
 {
+	dm_enter
 	return __send_changing_extent_only(ci, get_num_discard_bios,
 					   is_split_required_for_discard);
 }
 
 static int __send_write_same(struct clone_info *ci)
 {
+	dm_enter
 	return __send_changing_extent_only(ci, get_num_write_same_bios, NULL);
 }
 
@@ -1244,6 +1306,7 @@ static sector_t __len_within_target(stru
 	struct bio *bio = ci->bio;
 	sector_t bv_len, total_len = 0;
 
+	dm_enter
 	for (*idx = ci->idx; max && (*idx < bio->bi_vcnt); (*idx)++) {
 		bv_len = to_sector(bio->bi_io_vec[*idx].bv_len);
 
@@ -1266,6 +1329,7 @@ static int __split_bvec_across_targets(s
 	unsigned offset = 0;
 	sector_t len;
 
+	dm_enter
 	do {
 		if (offset) {
 			ti = dm_table_find_target(ci->map, ci->sector);
@@ -1300,6 +1364,7 @@ static int __split_and_process_non_flush
 	sector_t len, max;
 	int idx;
 
+	dm_enter
 	if (unlikely(bio->bi_rw & REQ_DISCARD))
 		return __send_discard(ci);
 	else if (unlikely(bio->bi_rw & REQ_WRITE_SAME))
@@ -1354,6 +1419,7 @@ static void __split_and_process_bio(stru
 	struct clone_info ci;
 	int error = 0;
 
+	dm_enter
 	ci.map = dm_get_live_table(md);
 	if (unlikely(!ci.map)) {
 		bio_io_error(bio);
@@ -1402,6 +1468,7 @@ static int dm_merge_bvec(struct request_
 	sector_t max_sectors;
 	int max_size = 0;
 
+	dm_enter
 	if (unlikely(!map))
 		goto out;
 
@@ -1459,6 +1526,7 @@ static void _dm_request(struct request_q
 	struct mapped_device *md = q->queuedata;
 	int cpu;
 
+	dm_enter
 	down_read(&md->io_lock);
 
 	cpu = part_stat_lock();
@@ -1484,6 +1552,7 @@ static void _dm_request(struct request_q
 
 static int dm_request_based(struct mapped_device *md)
 {
+	dm_enter
 	return blk_queue_stackable(md->queue);
 }
 
@@ -1491,6 +1560,7 @@ static void dm_request(struct request_qu
 {
 	struct mapped_device *md = q->queuedata;
 
+	dm_enter
 	if (dm_request_based(md))
 		blk_queue_bio(q, bio);
 	else
@@ -1501,6 +1571,7 @@ void dm_dispatch_request(struct request
 {
 	int r;
 
+	dm_enter
 	if (blk_queue_io_stat(rq->q))
 		rq->cmd_flags |= REQ_IO_STAT;
 
@@ -1518,6 +1589,7 @@ static int dm_rq_bio_constructor(struct
 	struct dm_rq_clone_bio_info *info =
 		container_of(bio, struct dm_rq_clone_bio_info, clone);
 
+	dm_enter
 	info->orig = bio_orig;
 	info->tio = tio;
 	bio->bi_end_io = end_clone_bio;
@@ -1531,6 +1603,7 @@ static int setup_clone(struct request *c
 {
 	int r;
 
+	dm_enter
 	r = blk_rq_prep_clone(clone, rq, tio->md->bs, GFP_ATOMIC,
 			      dm_rq_bio_constructor, tio);
 	if (r)
@@ -1552,6 +1625,7 @@ static struct request *clone_rq(struct r
 	struct request *clone;
 	struct dm_rq_target_io *tio;
 
+	dm_enter
 	tio = alloc_rq_tio(md, gfp_mask);
 	if (!tio)
 		return NULL;
@@ -1580,6 +1654,7 @@ static int dm_prep_fn(struct request_que
 	struct mapped_device *md = q->queuedata;
 	struct request *clone;
 
+	dm_enter
 	if (unlikely(rq->special)) {
 		DMWARN("Already has something in rq->special.");
 		return BLKPREP_KILL;
@@ -1606,6 +1681,7 @@ static int map_request(struct dm_target
 	int r, requeued = 0;
 	struct dm_rq_target_io *tio = clone->end_io_data;
 
+	dm_enter
 	tio->ti = ti;
 	r = ti->type->map_rq(ti, clone, &tio->info);
 	switch (r) {
@@ -1641,6 +1717,7 @@ static struct request *dm_start_request(
 {
 	struct request *clone;
 
+	dm_enter
 	blk_start_request(orig);
 	clone = orig->special;
 	atomic_inc(&md->pending[rq_data_dir(clone)]);
@@ -1669,6 +1746,7 @@ static void dm_request_fn(struct request
 	struct request *rq, *clone;
 	sector_t pos;
 
+	dm_enter
 	/*
 	 * For suspend, check blk_queue_stopped() and increment
 	 * ->pending within a single queue_lock not to increment the
@@ -1724,6 +1802,7 @@ out:
 
 int dm_underlying_device_busy(struct request_queue *q)
 {
+	dm_enter
 	return blk_lld_busy(q);
 }
 EXPORT_SYMBOL_GPL(dm_underlying_device_busy);
@@ -1734,6 +1813,7 @@ static int dm_lld_busy(struct request_qu
 	struct mapped_device *md = q->queuedata;
 	struct dm_table *map = dm_get_live_table(md);
 
+	dm_enter
 	if (!map || test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags))
 		r = 1;
 	else
@@ -1750,6 +1830,7 @@ static int dm_any_congested(void *conges
 	struct mapped_device *md = congested_data;
 	struct dm_table *map;
 
+	dm_enter
 	if (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {
 		map = dm_get_live_table(md);
 		if (map) {
@@ -1775,6 +1856,7 @@ static int dm_any_congested(void *conges
  *---------------------------------------------------------------*/
 static void free_minor(int minor)
 {
+	dm_enter
 	spin_lock(&_minor_lock);
 	idr_remove(&_minor_idr, minor);
 	spin_unlock(&_minor_lock);
@@ -1787,6 +1869,7 @@ static int specific_minor(int minor)
 {
 	int r;
 
+	dm_enter
 	if (minor >= (1 << MINORBITS))
 		return -EINVAL;
 
@@ -1806,6 +1889,7 @@ static int next_free_minor(int *minor)
 {
 	int r;
 
+	dm_enter
 	idr_preload(GFP_KERNEL);
 	spin_lock(&_minor_lock);
 
@@ -1825,6 +1909,7 @@ static void dm_wq_work(struct work_struc
 
 static void dm_init_md_queue(struct mapped_device *md)
 {
+	dm_enter
 	/*
 	 * Request-based dm devices cannot be stacked on top of bio-based dm
 	 * devices.  The type of this dm device has not been decided yet.
@@ -1853,6 +1938,7 @@ static struct mapped_device *alloc_dev(i
 	struct mapped_device *md = kzalloc(sizeof(*md), GFP_KERNEL);
 	void *old_md;
 
+	dm_enter
 	if (!md) {
 		DMWARN("unable to allocate device, out of memory.");
 		return NULL;
@@ -1951,6 +2037,7 @@ static void free_dev(struct mapped_devic
 {
 	int minor = MINOR(disk_devt(md->disk));
 
+	dm_enter
 	unlock_fs(md);
 	bdput(md->bdev);
 	destroy_workqueue(md->wq);
@@ -1976,6 +2063,7 @@ static void __bind_mempools(struct mappe
 {
 	struct dm_md_mempools *p = dm_table_get_md_mempools(t);
 
+	dm_enter
 	if (md->io_pool && md->bs) {
 		/* The md already has necessary mempools. */
 		if (dm_table_get_type(t) == DM_TYPE_BIO_BASED) {
@@ -2020,6 +2108,7 @@ static void event_callback(void *context
 	LIST_HEAD(uevents);
 	struct mapped_device *md = (struct mapped_device *) context;
 
+	dm_enter
 	spin_lock_irqsave(&md->uevent_lock, flags);
 	list_splice_init(&md->uevent_list, &uevents);
 	spin_unlock_irqrestore(&md->uevent_lock, flags);
@@ -2035,6 +2124,7 @@ static void event_callback(void *context
  */
 static void __set_size(struct mapped_device *md, sector_t size)
 {
+	dm_enter
 	set_capacity(md->disk, size);
 
 	i_size_write(md->bdev->bd_inode, (loff_t)size << SECTOR_SHIFT);
@@ -2051,6 +2141,7 @@ int dm_queue_merge_is_compulsory(struct
 {
 	struct mapped_device *dev_md;
 
+	dm_enter
 	if (!q->merge_bvec_fn)
 		return 0;
 
@@ -2070,6 +2161,7 @@ static int dm_device_merge_is_compulsory
 	struct block_device *bdev = dev->bdev;
 	struct request_queue *q = bdev_get_queue(bdev);
 
+	dm_enter
 	return dm_queue_merge_is_compulsory(q);
 }
 
@@ -2082,6 +2174,7 @@ static int dm_table_merge_is_optional(st
 	unsigned i = 0;
 	struct dm_target *ti;
 
+	dm_enter
 	while (i < dm_table_get_num_targets(table)) {
 		ti = dm_table_get_target(table, i++);
 
@@ -2105,6 +2198,7 @@ static struct dm_table *__bind(struct ma
 	unsigned long flags;
 	int merge_is_optional;
 
+	dm_enter
 	size = dm_table_get_size(t);
 
 	/*
@@ -2154,6 +2248,7 @@ static struct dm_table *__unbind(struct
 	struct dm_table *map = md->map;
 	unsigned long flags;
 
+	dm_enter
 	if (!map)
 		return NULL;
 
@@ -2172,6 +2267,7 @@ int dm_create(int minor, struct mapped_d
 {
 	struct mapped_device *md;
 
+	dm_enter
 	md = alloc_dev(minor);
 	if (!md)
 		return -ENXIO;
@@ -2188,26 +2284,31 @@ int dm_create(int minor, struct mapped_d
  */
 void dm_lock_md_type(struct mapped_device *md)
 {
+	dm_enter
 	mutex_lock(&md->type_lock);
 }
 
 void dm_unlock_md_type(struct mapped_device *md)
 {
+	dm_enter
 	mutex_unlock(&md->type_lock);
 }
 
 void dm_set_md_type(struct mapped_device *md, unsigned type)
 {
+	dm_enter
 	md->type = type;
 }
 
 unsigned dm_get_md_type(struct mapped_device *md)
 {
+	dm_enter
 	return md->type;
 }
 
 struct target_type *dm_get_immutable_target_type(struct mapped_device *md)
 {
+	dm_enter
 	return md->immutable_target_type;
 }
 
@@ -2218,6 +2319,7 @@ static int dm_init_request_based_queue(s
 {
 	struct request_queue *q = NULL;
 
+	dm_enter
 	if (md->queue->elevator)
 		return 1;
 
@@ -2242,6 +2344,7 @@ static int dm_init_request_based_queue(s
  */
 int dm_setup_md_queue(struct mapped_device *md)
 {
+	dm_enter
 	if ((dm_get_md_type(md) == DM_TYPE_REQUEST_BASED) &&
 	    !dm_init_request_based_queue(md)) {
 		DMWARN("Cannot initialize queue for request-based mapped device");
@@ -2256,6 +2359,7 @@ static struct mapped_device *dm_find_md(
 	struct mapped_device *md;
 	unsigned minor = MINOR(dev);
 
+	dm_enter
 	if (MAJOR(dev) != _major || minor >= (1 << MINORBITS))
 		return NULL;
 
@@ -2279,6 +2383,7 @@ out:
 struct mapped_device *dm_get_md(dev_t dev)
 {
 	struct mapped_device *md = dm_find_md(dev);
+	dm_enter
 
 	if (md)
 		dm_get(md);
@@ -2289,22 +2394,26 @@ EXPORT_SYMBOL_GPL(dm_get_md);
 
 void *dm_get_mdptr(struct mapped_device *md)
 {
+	dm_enter
 	return md->interface_ptr;
 }
 
 void dm_set_mdptr(struct mapped_device *md, void *ptr)
 {
+	dm_enter
 	md->interface_ptr = ptr;
 }
 
 void dm_get(struct mapped_device *md)
 {
+	dm_enter
 	atomic_inc(&md->holders);
 	BUG_ON(test_bit(DMF_FREEING, &md->flags));
 }
 
 const char *dm_device_name(struct mapped_device *md)
 {
+	dm_enter
 	return md->name;
 }
 EXPORT_SYMBOL_GPL(dm_device_name);
@@ -2313,6 +2422,7 @@ static void __dm_destroy(struct mapped_d
 {
 	struct dm_table *map;
 
+	dm_enter
 	might_sleep();
 
 	spin_lock(&_minor_lock);
@@ -2347,16 +2457,19 @@ static void __dm_destroy(struct mapped_d
 
 void dm_destroy(struct mapped_device *md)
 {
+	dm_enter
 	__dm_destroy(md, true);
 }
 
 void dm_destroy_immediate(struct mapped_device *md)
 {
+	dm_enter
 	__dm_destroy(md, false);
 }
 
 void dm_put(struct mapped_device *md)
 {
+	dm_enter
 	atomic_dec(&md->holders);
 }
 EXPORT_SYMBOL_GPL(dm_put);
@@ -2366,6 +2479,7 @@ static int dm_wait_for_completion(struct
 	int r = 0;
 	DECLARE_WAITQUEUE(wait, current);
 
+	dm_enter
 	add_wait_queue(&md->wait, &wait);
 
 	while (1) {
@@ -2398,6 +2512,7 @@ static void dm_wq_work(struct work_struc
 						work);
 	struct bio *c;
 
+	dm_enter
 	down_read(&md->io_lock);
 
 	while (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {
@@ -2423,6 +2538,7 @@ static void dm_wq_work(struct work_struc
 
 static void dm_queue_flush(struct mapped_device *md)
 {
+	dm_enter
 	clear_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);
 	smp_mb__after_clear_bit();
 	queue_work(md->wq, &md->work);
@@ -2437,6 +2553,7 @@ struct dm_table *dm_swap_table(struct ma
 	struct queue_limits limits;
 	int r;
 
+	dm_enter
 	mutex_lock(&md->suspend_lock);
 
 	/* device must be suspended */
@@ -2479,6 +2596,7 @@ static int lock_fs(struct mapped_device
 {
 	int r;
 
+	dm_enter
 	WARN_ON(md->frozen_sb);
 
 	md->frozen_sb = freeze_bdev(md->bdev);
@@ -2495,6 +2613,7 @@ static int lock_fs(struct mapped_device
 
 static void unlock_fs(struct mapped_device *md)
 {
+	dm_enter
 	if (!test_bit(DMF_FROZEN, &md->flags))
 		return;
 
@@ -2526,6 +2645,7 @@ int dm_suspend(struct mapped_device *md,
 	int do_lockfs = suspend_flags & DM_SUSPEND_LOCKFS_FLAG ? 1 : 0;
 	int noflush = suspend_flags & DM_SUSPEND_NOFLUSH_FLAG ? 1 : 0;
 
+	dm_enter
 	mutex_lock(&md->suspend_lock);
 
 	if (dm_suspended_md(md)) {
@@ -2628,6 +2748,7 @@ int dm_resume(struct mapped_device *md)
 	int r = -EINVAL;
 	struct dm_table *map = NULL;
 
+	dm_enter
 	mutex_lock(&md->suspend_lock);
 	if (!dm_suspended_md(md))
 		goto out;
@@ -2671,6 +2792,7 @@ int dm_kobject_uevent(struct mapped_devi
 	char udev_cookie[DM_COOKIE_LENGTH];
 	char *envp[] = { udev_cookie, NULL };
 
+	dm_enter
 	if (!cookie)
 		return kobject_uevent(&disk_to_dev(md->disk)->kobj, action);
 	else {
@@ -2683,16 +2805,19 @@ int dm_kobject_uevent(struct mapped_devi
 
 uint32_t dm_next_uevent_seq(struct mapped_device *md)
 {
+	dm_enter
 	return atomic_add_return(1, &md->uevent_seq);
 }
 
 uint32_t dm_get_event_nr(struct mapped_device *md)
 {
+	dm_enter
 	return atomic_read(&md->event_nr);
 }
 
 int dm_wait_event(struct mapped_device *md, int event_nr)
 {
+	dm_enter
 	return wait_event_interruptible(md->eventq,
 			(event_nr != atomic_read(&md->event_nr)));
 }
@@ -2700,6 +2825,7 @@ int dm_wait_event(struct mapped_device *
 void dm_uevent_add(struct mapped_device *md, struct list_head *elist)
 {
 	unsigned long flags;
+	dm_enter
 
 	spin_lock_irqsave(&md->uevent_lock, flags);
 	list_add(elist, &md->uevent_list);
@@ -2712,11 +2838,13 @@ void dm_uevent_add(struct mapped_device
  */
 struct gendisk *dm_disk(struct mapped_device *md)
 {
+	dm_enter
 	return md->disk;
 }
 
 struct kobject *dm_kobject(struct mapped_device *md)
 {
+	dm_enter
 	return &md->kobj;
 }
 
@@ -2728,6 +2856,7 @@ struct mapped_device *dm_get_from_kobjec
 {
 	struct mapped_device *md;
 
+	dm_enter
 	md = container_of(kobj, struct mapped_device, kobj);
 	if (&md->kobj != kobj)
 		return NULL;
@@ -2742,17 +2871,20 @@ struct mapped_device *dm_get_from_kobjec
 
 int dm_suspended_md(struct mapped_device *md)
 {
+	dm_enter
 	return test_bit(DMF_SUSPENDED, &md->flags);
 }
 
 int dm_suspended(struct dm_target *ti)
 {
+	dm_enter
 	return dm_suspended_md(dm_table_get_md(ti->table));
 }
 EXPORT_SYMBOL_GPL(dm_suspended);
 
 int dm_noflush_suspending(struct dm_target *ti)
 {
+	dm_enter
 	return __noflush_suspending(dm_table_get_md(ti->table));
 }
 EXPORT_SYMBOL_GPL(dm_noflush_suspending);
@@ -2764,6 +2896,7 @@ struct dm_md_mempools *dm_alloc_md_mempo
 	unsigned int pool_size;
 	unsigned int front_pad;
 
+	dm_enter
 	if (!pools)
 		return NULL;
 
@@ -2801,6 +2934,7 @@ out:
 
 void dm_free_md_mempools(struct dm_md_mempools *pools)
 {
+	dm_enter
 	if (!pools)
 		return;
 
Index: linux-latency/include/linux/device-mapper.h
===================================================================
--- linux-latency.orig/include/linux/device-mapper.h
+++ linux-latency/include/linux/device-mapper.h
@@ -18,6 +18,8 @@ struct dm_table;
 struct mapped_device;
 struct bio_vec;
 
+#define dm_enter	do{printk("%s:%d enter %s\n", __FILE__, __LINE__, __func__);}while(0);
+
 typedef enum { STATUSTYPE_INFO, STATUSTYPE_TABLE } status_type_t;
 
 union map_info {
